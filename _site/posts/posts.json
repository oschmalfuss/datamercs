[
  {
    "path": "posts/welcome/",
    "title": "Welcome to OS DataMercs",
    "description": "OS DataMercs is inspired to elevate the publishing industry by consulting, encouraging and providing most recognized metadata services.",
    "author": [
      {
        "name": "Olaf Schmalfuß",
        "url": "https://www.datamercs.net"
      }
    ],
    "date": "2022-09-01",
    "categories": [],
    "contents": "\r\n// Trinity of Discovery\r\nThe “Trinity of Discovery” exhibits our holistic approach to metadata\r\ndelivery and creation, and sets the framework to a title’s - may it be\r\neBook or journal - (meta) data needs.\r\nIt is also what most libraries would expect to receive from a publisher\r\none way or another, either directly or through third party services or\r\nunion catalogues:\r\nmeta data and discovery from the smallest chunk down to collection\r\nlevel, because\r\n\r\n»We are not going to buy any of your eBooks if users cannot discover\r\nthem.«\r\n(an unnamed head of aqcuisition)\r\n\r\nMARC records, KBARTs, ONIX and JATS XML are key, especially to\r\nscientific publishing houses!\r\nNaturally, one could go beyond that, if we talked about linked data,\r\nwhich would even aim at the most atomic bits of data points, but since\r\nmost publishers usually have hardly even the basics covered, the\r\ntrinity shall serve as our North star here.\r\nThus, this site aims to showcase several tools and workflows around\r\nthat, so that even the smallest publisher, or inclined librarian, can\r\nbuild upon that and implement their own (half-) automated data\r\npipelines, from ERP to .MRC;-)\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/welcome/trinity of discovery.jpg",
    "last_modified": "2022-09-01T10:58:24+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-28-rough-guide-to-git-large-file-storage-lfs/",
    "title": "rough guide to Git Large File Storage (LFS)",
    "description": "Git Large File Storage (LFS) allows for versioning files larger than 100MB on GitHub. While this seems like a no brainer at first glance, it also comes with some draw backs. Thus, here's a rough guide on howto set-up Git-LFS and uninstall from a project again.",
    "author": [
      {
        "name": "Olaf Schmalfuss",
        "url": "https://www.datamercs.net"
      }
    ],
    "date": "2022-02-28",
    "categories": [],
    "contents": "\r\n\r\nContents\r\n//\r\nIntroduction\r\n// Set-up\r\n// commit and push to GitHub\r\n// check files\r\n// push all referenced\r\nGit-LFS files\r\n\r\n//\r\nIssues with Git-LFS\r\n// how to uninstall Git-LFS\r\n\r\n\r\n// Introduction\r\nGitHub limits the size of files allowed in repositories to up to\r\n100MB. For working with larger files like data sets or binaries such as\r\nMARC record sets, we have to find a solution around that limit. In comes\r\nGit Large File Storage\r\n(LFS)↗.\r\nGit-LFS is an open source Git extension for versioning files above 100MB\r\nby replacing them with text pointers inside Git, while storing the file\r\ncontents outside of the normal Git project on a remote server like\r\nGitHub.com.\r\nsee more info here: https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github\r\n// Set-up\r\ndownload Git-LFS here https://git-lfs.github.com/ and install.\r\nopen Git Bash\r\nverify that the installation was successful:\r\n\r\n$ git lfs install\r\n> Git LFS initialized.\r\n\r\ncd into the repository’s directory we’d like to use\r\nwith Git-LFS.\r\nselect the file types or files we’d like Git-LFS to manage (or\r\ndirectly edit .gitattributes)\r\nassociate a whole file type, e.g. .ZIP, with Git-LFS by file\r\nextension:\r\n\r\n# e.g. associate all .ZIP files with Git-LFS:\r\n\r\n$ git lfs track \"*.zip\"\r\n> Adding path *.zip\r\n\r\nassociate a single file with Git-LFS:\r\n\r\n$ git lfs track --filename [path to file]\r\n> Tracking \"[path to file]\"\r\n\r\n// commit and push to GitHub\r\n\r\n$ git add [path to file]\r\n$ git commit -m \"update MARC\"\r\n$ git push origin main\r\n\r\n// check files\r\nlist all the (large) files manage by Git-LFS.\r\n\r\n$ cd [path to repository]\r\n$ git lfs ls-files\r\n\r\n// push all referenced\r\nGit-LFS files\r\n\r\n$ git lfs push --all origin\r\n\r\n// Issues with Git-LFS\r\nSo while on paper we get the benefit of being able to handle 100MB+\r\nfiles, Git-LFS also suddenly adds limitations to your repository’s total\r\nsize as well as to the bandwidth, i.e. 1GB each, resulting in the\r\nfollowing error message:\r\n\r\nUploading LFS objects: 0% (0/1), 0 B | 0 B/s, done. batch response:\r\nThis repository is over its data quota. Account responsible for LFS\r\nbandwidth should purchase more data packs to restore access.\r\n\r\nThis can happen really fast, especially if you have lots of med sized\r\nfiles and work not as organized or efficient in your repository … as an\r\nlibrarian maybe…\r\nTherefore, it is also good to know how to uninstall Git-LFS and start\r\nover more organized moving forward.\r\nThe other solution would be a paid subscription.\r\n// how to uninstall Git-LFS\r\nSimply removing the files from the project does not work, as the\r\nGit-LFS objects still exist on the remote storage and will continue to\r\ncount toward the Git-LFS storage quota. To remove Git-LFS objects from a\r\nrepository, delete and recreate the repository.\r\nremove LFS hooks\r\n\r\n$ git lfs uninstall\r\n\r\nremove the LFS filters from .gitattributes\r\nlist LFS files using\r\n\r\n$ git lfs ls-files | sed -r 's/^.{13}//' > lfs_files.txt\r\n\r\nrun git rm –cached for each file\r\n\r\nwhile read line; do\r\n  git rm --cached \"$line\"\r\ndone < lfs_files.txt\r\n\r\nrun git add for each file\r\n\r\nwhile read line; do\r\n  git add \"$line\"\r\ndone < lfs_files.txt\r\n\r\ncommit everything\r\n\r\n$ git add .gitattributes\r\n$ git commit -m \"de-lfs\"\r\n$ git push origin\r\n\r\ncheck that no LFS files remain\r\n\r\n$ git lfs ls-files\r\n\r\nremove LFS cache and temp file\r\n\r\n$ rm -rf .git/lfs lfs_files.txt\r\n\r\net voilà\r\n// new & easy solution\r\n\r\n\r\n$ git lfs uninstall\r\n\r\n# then manually remove the LFS filters from .gitattributes\r\n\r\n$ git lfs untrack \"*.zip\"\r\n$ git add --renormalize \r\n$ git commit -m \"de-lfs\"\r\n$ git push origin\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-02-28-rough-guide-to-git-large-file-storage-lfs/GITLFS.png",
    "last_modified": "2022-09-01T11:09:29+02:00",
    "input_file": "rough-guide-to-git-large-file-storage-lfs.knit.md",
    "preview_width": 511,
    "preview_height": 272
  },
  {
    "path": "posts/2021-12-18-a-rough-guide-to-vpns/",
    "title": "A rough guide to VPNs",
    "description": "Since YouTube influencer marketing you know you need a VPN (i.e. virtual private network). There many of services, mostly commercial, but also some free and academic options.  \nWhile VPNs are a solid solution in an institutional environment to access one's company's network, or for circumventing censorship or geo-blocking, there're wild claims being made when it comes to potential capabilities of VPNs or internet threats in general.  \nThis article is not intended to deeply elaborate on that, but serves as a personal memo as well as a *thought piece*.",
    "author": [
      {
        "name": "Olaf Schmalfuss",
        "url": "https://www.datamercs.net"
      }
    ],
    "date": "2021-12-18",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nVPN Gate\r\nVPN\r\nGate server list\r\nVPN Client\r\n\r\nReality check - why\r\nyou shouldn’t trust VPNs\r\nBut what’s the solution to\r\nVPNs?\r\nAll you\r\nneed to know about Very Pwnable Networks (VPNs)\r\n\r\n\r\nIntroduction\r\nSince YouTube influencer marketing you know you need a VPN\r\n(i.e. virtual private network). There are lots of services, mostly\r\ncommercial after all, but also some free and academic options.\r\nWhile VPNs are definitely a solution in an institutional/ company\r\nsetting to access one’s company’s intranet remotely, or for safely\r\ncircumventing national censorship and geo-blocking by streaming or\r\nnational broadcasting services, there’re wild claims being made when it\r\ncomes to potential capabilities of VPNs or use cases.\r\nThis article is not intended to deeply elaborate on that, but serves\r\nas a personal memo as well as a small thought piece in the sanity\r\ncheck below.\r\nFor more knowledgeable information on VPNs, see the EFF’s article on Choosing\r\nthe VPN That’s Right for You↗.\r\nVPN Gate\r\nVPN Gate\r\nAcademic Experiment Project↗ is an online service as an academic\r\nresearch at Graduate School of University of Tsukuba, Japan. The purpose\r\nof this research is to expand the knowledge of “Global Distributed\r\nPublic VPN Relay Servers”.\r\nThe project is not only for consumption, but anyone is encouraged to participate↗.\r\nDo we trust it? No. Do we use it? For our purposes, yes.\r\nVPN Gate server list\r\nlist of 7000+ public VPN relay servers by volunteers around the\r\nworld:\r\n\r\nhttps://www.vpngate.net/en/\r\n\r\n\r\nU/N: vpn\r\nP/W: vpn\r\n\r\nVPN Client\r\nSoftEther VPN↗ is an\r\nOpen-Source, free, cross-platform multi-protocol VPN program, developed\r\nas an academic project from University of Tsukuba↗, under the\r\nApache License 2.0.\r\nDownload, install and set-up:\r\n\r\nhttps://github.com/SoftEtherVPN/SoftEtherVPN\r\n\r\nReality check - why\r\nyou shouldn’t trust VPNs\r\nVPN advertisements are common on social media these days and making\r\nthe wildest claims about privacy, security and anonymity. Basically just\r\nwith the click of a button, you’re safe and virtually invisible on the\r\ninternet, surfing “through an unbreakable tunnel that no one can look\r\ninto, be it cybercriminals, governments, or internet service\r\nproviders”!?\r\nNaturally, this is not true, as technically one is basically only\r\nchanging IPs1 and very importantly WHO is\r\ntracking your internet usage now. Switching (trust) from a local\r\nInternet Service provider (ISP) to another third party service with\r\nusually unclear jurisdictions.\r\nOf course, all VPN providers will claim that according to their\r\npolicy they’d encrypt your data, never keep any history logs, DNS\r\nrequests or other personal information and whatnot, but one should treat\r\nthat as nothing more than marketing, especially when the service has any\r\nties to any of the Five/ Fourteen Eyes alliance (FVEY↗) members and\r\nask: who hasn’t!?\r\nThis question is relevant, because (remember Upstream and\r\nPRISM↗!?) companies operating from any of the alliance states can be\r\nforced to spy on users without having any legal way to disclose that\r\nfact to their users, possibly bound with a gag order that cannot be\r\nviolated without risking severe legal consequences\r\n(i.e. Prism). In upstream surveillance, the agencies’\r\npartners like AT&T tap into the wires directly and copy all of the\r\ndata flowing through the data highway. Of course, the companies will\r\ncomply, while marketing-wise they’ll phrase it such they didn’t.\r\nMoreover, even if a service operates from a safe haven, what about their\r\n(physical) infrastructure: does the VPN provider actually OWN those\r\nmachines? Or are they rather outsourced and rented from a 3rd party\r\nlocated in a 3rd (or even 4th) party network, or running services on\r\nvirtual machines hosted by third parties? Does anyone believe that VPN\r\nproviders can easily offer exit machines in two dozen countries by\r\ndeploying hardware they own and operate in datacentres they can\r\ntrust?\r\nIn an ideal setting, your VPN provider owns all of the hardware AND the\r\nnetwork it operates, yet, after all, most VPN providers are primarily\r\nbusinesses! And the primary objective of most businesses is revenue and\r\ndriving costs down…\r\nAs a result, one of the easiest ways to infiltrate and exploit VPN\r\ninfrastructure is simply to offer cheap (possibly even tax-payer\r\nsubsidised) server hosting, “tailored for your company’s needs”.\r\nAnd even worse, some “VPN” services are simply outright scams↗.\r\nBesides unscrupulous organizations that make online privacy harder\r\nfor the general population, like the various government agencies\r\nwiretapping and breaking into systems to steal our data, or ISP/ VPN\r\nservices that mess with user traffic and inject ads to earn some extra\r\nrevenue by exploiting their customers for advertising, the technology\r\nitself is not inherently safe easy to deal with either, e.g. due to\r\nDNS leaks↗, IPv6 leaks↗\r\nProblems with the cryptographic routines used, like\r\nOpenSSL↗,\r\nsuch as Heartbleed, POODLE, FREAK, Logjam etc.\r\nthe literal VULCANDEATHGRIP↗\r\n…\r\n\r\nBrowser related leaks↗\r\nand bad crypto config↗\r\nUn- or improperly configured firewalls\r\nbrowser\r\nfingerprinting↗, ad- and social media network tracking\r\nmalicious websites or compromised networks\r\netc\r\nAgain, for more knowledgeable information see the EFF’s guide on Choosing\r\nthe VPN That’s Right for You↗.\r\nBut what’s the solution to\r\nVPNs?\r\ndon’t fall for it and manage your expectations accordingly\r\nTOR↗ might come handy as\r\nwell\r\nTreat every service on the internet as if they had an API to Big\r\nBrother, which they most likely do. While it looks inconspicuous on the\r\nfront end, in the back-end your data might already be consolidated.\r\noperate in meat-space\r\nvisit a library!\r\nAll you\r\nneed to know about Very Pwnable Networks (VPNs)\r\nWhile primarily intended as advertisements for their VPN service “IPredator”,\r\nco-founded by Pirate Bay co-founder Peter Sunde, the comics actually\r\nraised some valid points to consider, that obviously largely served as\r\nthe inspiration for the above.\r\nAs neither their site nor their services are available any longer, this\r\n“comic” (or “textbook”!?) can now be found below again for educational\r\npurposes:\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nUsing a VPN masks the IP address\r\nassigned by your ISP from the sites that you access, adding a layer of\r\nprivacy. Along with masking your origin IP address, it also encrypts\r\nyour data while in transit to the site you are accessing, which actually\r\nis useful especially when connected to an insecure Wi-Fi network in a\r\ncafé, airport, library, or somewhere else.↩︎\r\n",
    "preview": "posts/2021-12-18-a-rough-guide-to-vpns/distill-preview.png",
    "last_modified": "2022-09-01T10:54:44+02:00",
    "input_file": {},
    "preview_width": 654,
    "preview_height": 652
  },
  {
    "path": "posts/2021-09-28-zoll-einfuhrabgaben-dhl-auslagepauschale-reklamation/",
    "title": "Wie erhalte ich falsch erhobene Zoll-Einfuhrabgaben sowie die DHL-Auslagepauschale zurück?",
    "description": "Seitdem am 1. Juli 2021 die bisherige 22-Euro-Freigrenze für Waren aus dem Nicht-EU-Ausland entfiel, werden scheinbar auch Geschenksendungen, die weiter bis zu einem Wert von 45 EUR zoll- und einfuhrumsatzsteuerfrei sind, systematisch abgerechnet, zumal die Post den \"Service\" der Zollanmeldung übernommen hat und jegliche Gebühren - plus 6 EUR Auslagenpauschale - fraglos vorstreckt und dann per Postboten eintreibt, auch wenn die Pakete klar als Geschenk gekennzeichnet sind.  \nHier wird erklärt, wie man sich Zoll und DHL-Auslagenpauschale in drei Schritten zurückholt.",
    "author": [
      {
        "name": "Olaf Schmalfuß",
        "url": "https://www.datamercs.net"
      }
    ],
    "date": "2021-09-28",
    "categories": [],
    "contents": "\r\n\r\nContents\r\n// Einführung - Zoll und\r\nDHL Hand in Hand\r\n// 0.\r\nDokumentation\r\n// 1. original\r\nDHL-Abgabenbescheid anfordern\r\n// 2. Zoll - Erlass-\r\nbzw. Erstattungsverfahren\r\n//\r\n3. DHL-Auslagenpauschale\r\n\r\n// Einführung - Zoll und\r\nDHL Hand in Hand\r\nFehler oder Kalkül? Ab dem 1. Juli 2021 entfiel (offenbar zur\r\nEindämmung des Online-Geschäfts via Alibaba & co.) die bisherige\r\n22-Euro-Freigrenze für Waren aus dem Nicht-EU-Ausland nach Deutschland,\r\nsodass für alle Warensendungen (bis auf wenige Ausnahmen) nun Abgaben\r\nfällig werden1.\r\nGleichzeitig übernahm ganz service-orientiert die Deutsche Post “als\r\nIhr Transportdienstleister” die Zollanmeldung für alle derartigen\r\nWarensendungen (gegen 6 EUR Auslagenpauschale), und so scheinen nun\r\nvöllig ungeprüft einfach alle Auslandssendungen ungeachtet jeglicher\r\nZollinhaltserklärung erstmal systematisch verzollt und nicht mehr wie\r\nzuvor, im Falle klar gekennzeichneter Geschenksendungen, regulär durch-\r\noder wenigstens zur etwaigen Klärung zum lokalen Zollamt geschickt zu\r\nwerden - so zumindest in 100% aller Pakete seither hier.\r\nDas spart natürlich Aufwand und der Zoll kann jetzt erst einmal\r\nwiderspruchslos kassieren. Die Post streicht die 6 EUR pro Paket extra\r\nein und der Empfänger hat das Nachsehen, denn zur Reklamation wird der\r\nnun zu leistende (Zeit-) Aufwand die Rückforderung i.d.R. wohl\r\nübersteigen:-\\\r\nUnd so erweist sich auch für uns diese Regelung als recht ungünstig,\r\nda die Oma aus dem Ausland ihren Enkeln gerne einmal kleinere Care-Pakete↗ zukommen\r\nlässt, sodass selbst bei eigentlich zu vernachlässigenden Zollbeträgen\r\njew. noch die besagten 6 EUR DHL-Auslagenpauschale hinzukommen, wo das\r\nNachsehen dann doch ein Ende hat. Denn weiterhin gilt:\r\n\r\n“Geschenksendungen im Wert von bis zu 45 Euro sind zoll- und\r\neinfuhrumsatzsteuerfrei 2.\r\n[ABER:] Die Befreiung von den Einfuhrabgaben kann nur gewährt werden,\r\nwenn dies durch den Post- und Kurierdienstleister beantragt wird. Die\r\nZollverwaltung geht in der Regel von einer ordnungsgemäßen Zollanmeldung\r\nder Post- und Kurierdienstleister aus.3”\r\n\r\nDas heißt, an irgendeiner Stelle hätte wohl die ordnungsgemäße\r\nVerarbeitung auch für Geschenksendungen durchaus stattfinden sollen,\r\nwobei die KI offenkundlich sogar chin. Adressen maschinell auslesen\r\nkönnen soll, aber für standardisierte Häkchen auf einem CN22-Formular\r\ndie entsprechende Zeile im OCR-Code sicherlich nur versehentlich\r\nauskommentiert wurde4, o.ä. … Ein Schelm, der Böses dabei\r\ndenkt!\r\nDaher soll an dieser Stelle der Prozess für die Rückforderung(en) zur\r\nkünftigen Referenz einmal festgehalten werden, nicht zuletzt, um es den\r\no.g. Institutionen nicht zu einfach zu machen, und letztlich handelt es\r\nsich auch “nur” um drei Schritte:\r\nfür einen “Antrag auf Erstattung oder Erlass” beim Zoll benötigen\r\nwir von der Post zunächst den originalen DHL-Abgabenbescheid↗:\r\n\r\n“Wenn Sie gegen die erhobenen Zollgebühren bzw. Einfuhrabgaben\r\nEinspruch einlegen möchten, wenden Sie sich bitte an das\r\nzuständige Zollamt. Die Kontaktdaten finden Sie auf dem\r\njeweiligen Abgabebescheid. Bitte beachten Sie, dass ein\r\nEinspruch innerhalb von 4 Wochen ab Festlegung des\r\nAbgabebescheids erfolgen muss. […]\r\nFalls Ihnen noch kein Abgabebescheid vorliegt, können Sie diesen unter\r\ndhl.de/einfuhr↗\r\nanfordern oder Sie scannen den QR-Code. Dieser befindet sich auf Ihrer\r\nSendung beim Aufkleber Einfuhrabgabenbescheid.”\r\n\r\ndamit stellen wir beim Zoll (per Formular\r\n0223↗ u.a.) den “Antrag\r\nauf Erstattung oder Erlass”↗ via Email und erhalten neben der\r\nEinfuhrabgabenrückzahlung auch einen korrigierten\r\nEinfuhrabgabenbescheid\r\nper Ticket↗\r\nkann bei DHL dann die Auslagenpauschale zurückgefordert werden:\r\n\r\n“Wir erstatten Ihnen dann gerne die Auslagenpauschale in Höhe von\r\n6,00 EUR, sobald wir den geänderten Einfuhrabgabenbescheid von Ihnen\r\nerhalten haben. Diesen können Sie uns ganz einfach per E-Mail zukommen\r\nlassen.”\r\n\r\nWas vorher also zwischen Zoll und DHL quasi über den Schreibtisch\r\nlief, muss andersherum nun natürlich über div. bürokratische und\r\ntechnische Hürden separat zurückgedreht werden…\r\nAlternativ kann man sich auch als “Selbstverzoller” registrieren\r\nlassen, was aber schließlich auch (ggf. wg. Zeit und Benzin wieder mehr)\r\nRessourcen verbraucht: https://www.dhl.de/de/privatkunden/hilfe-kundenservice/sendungsverfolgung/formular-selbstverzollung.html\r\n// 0. Dokumentation\r\nNoch bevor man das Paket auspackt, sollte idealerweise schon für\r\nSchritt drei das gesamte Paket von allen Seiten fotografiert werden.\r\nSinn oder Unsinn, aber das wird für den Antrag gefordert und ohne könnte\r\nder “Formfehler” wohl im schlechtesten Fall - wenn auch hier bislang\r\nunbelegt - evtl. die Rückforderung der DHL Auslagenpauschale\r\ngefährden.\r\nWeiter digitalisieren wir die Customs Declaration CN22 für die Anfrage\r\nbeim Zoll in Schritt zwei.\r\n// 1. original\r\nDHL-Abgabenbescheid anfordern\r\nhttps://www.dhl.de/de/privatkunden/hilfe-kundenservice/formulare/abgabenbescheid-anfordern.html\r\n\r\nNachname\r\nVorname\r\nStraße\r\nHausnummer\r\nPostleitzahl\r\nOrt\r\n(DHL)Sendungsnummer\r\n\r\n// 2. Zoll - Erlass-\r\nbzw. Erstattungsverfahren\r\nhttps://www.zoll.de/DE/Fachthemen/Zoelle/Abgabenerhebung/Erlass-Erstattung/Verfahren/verfahren_node.html\r\n2.1 “Antrag auf Erstattung oder Erlass” (Formular 0223) s. https://www.formulare-bfinv.de/ffw/action/invoke.do?id=0223\r\n2.2 der Zoll braucht eine Einwillingung für die “Elektronische\r\nKommunikation mit der Zollverwaltung gemäß § 87a Abgabenordnung”, damit\r\ndie Kommunikatione über Email erfolgen kann.\r\nEin Muster gibt es hier, worin jedoch das Hauptzollamt Köln bereits\r\nvorausgefüllt ist, wohingegen internat. Pakete wohl meist über Frankfurt\r\na.M. reinommen:https://media.frag-den-staat.de/files/foi/497520/MusterschreibenzurEinwilligungnach87aAbs.1Satz3AO.pdf\r\nDas (wohlformatierte) PDF kann aber in OpenOffice (oder Word) geöffnet\r\nund entsprechend umgeschrieben werden.\r\n2.3 Der Antrag selbst erfolgt dann per Email an die Adresse aus dem\r\nDHL-Abgabenbescheid, z.B.:\r\n\r\nto: poststelle.hza-ffm@zoll.bund.de\r\nsubject: Antrag auf Erstattung ZP130120223302/12345, Sendung\r\nUN029912345JP (DHL 64028612345)\r\n\r\n\r\nSehr geehrte Damen und Herren,\r\nhiermit möchte ich Einspruch gegen den m.E. fehlerhaften Zollbescheid\r\nZP130120223302/12345 vom 26.07.2021 für die Sendung UN029912345JP >\r\n(DHL 64028612345) einlegen:\r\nBei der Sendung handelt es sich um ein klar gekennzeichnetes Geschenk,\r\ni.e. ein Buch von [Sender] für [Empfänger] (s.\r\nAnhang).\r\nDer Warenwert wurde von Ihnen auf 37,50 EUR festgesetzt, und dann eine\r\nEUSt von 7,12 EUR erhoben.\r\nAber „Geschenksendungen im Wert von bis zu 45 Euro sind zoll- und\r\neinfuhrumsatzsteuerfrei“:https://www.zoll.de/DE/Privatpersonen/Postsendungen-Internetbestellungen/Sendungen-aus-einem-Nicht-EU-Staat/Zoll-und-Steuern/Geschenksendungen/geschenksendungen_node.html\r\nDaher möchten wir Sie\r\n1. um die Erstattung Einfuhrabgaben sowie\r\n2. um einen korrigierten Einfuhrabgabenbescheid\r\nbitten, sodass wir uns auch die Auslagepauschale in Höhe von 6,00 EUR\r\nbei der DHL erstatten lassen können.\r\nGeldinstitut: Hausbank\r\nIBAN: DE49112233445566\r\nBIC: HBKDEXXX\r\nFür Rückfragen und weitere Informationen stehen wir Ihnen jederzeit\r\nzur Verfügung.\r\nMit freundlichen Grüßen,\r\n[Empfänger]\r\nAnlagen:\r\n> Formular 0223 (0223.pdf)\r\n> Einwilligung in die unverschlüsselte elektronische Kommunikation\r\nper E-Mail gemäß § 87a Abgabenordnung\r\n(MusterschreibenzurEinwilligungnach87aAbs.1Satz3AO.pdf)\r\n> original DHL-Abgabenbescheid (DHL-Abgabenbescheid.pdf)\r\n> Customs Declaration CN22 (Customs_Declaration_CN22.jpg)\r\n\r\n// 3. DHL-Auslagenpauschale\r\nSobald der korrigierte Einfuhrabgabenbescheid vorliegt, kann für die\r\nSendung bei der DHL hier↗\r\nein neues “Anliegen” eröffnet werden:\r\n\r\n“bitte ein neues Anliegen eröffnen. Sobald Sie hier alle Unterlagen\r\nhaben nutzen Sie folgenden Link https://www.dhl.de/de/privatkunden/hilfe-kundenservice/formulare/formular-nachentgelt.html.”\r\n\r\nDieser offizielle Weg scheint in der Praxis aber nur bedingt zu\r\nfunktionieren, weshalb wir alternativ mit Bezug auf einen vergangenen\r\nFall ggf. wieder per Email anfragen (s.u.).\r\nEs werden künstliche Hürden eingebaut, wie etwa maximale Dateigrößen\r\nvon 1MB, was bei Handyphotos leicht überschritten wird.\r\nEs werden Photos aller sechs Seiten des Paketes, Label und Aufschriften\r\ngefordert5, aber nach fünf Einzel-Uploads\r\nverschwindet der Upload-Button für die Dateiauswahl…\r\nAlso müssen alle Photos auf Größe gebracht und entweder in eine\r\nPDF-Datei oder in eine Collage konvertiert werden.\r\nWie das unter bash ginge, einmal hier. Wir benötigen dazu\r\ndas Paket ImageMagick:\r\n\r\n# resize pictures to max file size, e.g. 1MB\r\nfor pic in *.jpg; do\r\n  convert $pic -define jpeg:extent=1024kb \"$(basename $pic .jpg)_res.jpg\"\r\ndone\r\n\r\n\r\n# collage, landscape, 3 columns x 2 rows \r\nmontage -mode concatenate -tile 3x2 -geometry +30+30 *.jpg -gravity center -background none -bordercolor none -define jpeg:extent=1024kb Sendung_64028612345_6-Seiten.jpg\r\n\r\nWeiter laden wir hier wieder die Zollinhaltserklärung CN 23 bei\r\nPaketen bzw. CN 22 bei Päckchen hoch.\r\nUnter Angabe der vollst. Sender-, Empfänger- sowie unserer\r\nKontoinformationen erhalten wir schließlich nach Absenden des Formulars,\r\nsofern über die ganzen Dateioperationen noch kein Timeout erreicht\r\nwurde, (idealerweise) die folgende Bestätigung an die angegebene\r\nEmail-Adresse:\r\n\r\nVielen Dank! Guten Tag, vielen Dank für Ihre Nachricht. Aufgrund der\r\naktuell unerwartet hohen Anzahl6 von\r\nAnfragen kann sich die Bearbeitung Ihres Anliegens verzögern. Auch wenn\r\nes derzeit etwas länger dauert, werden wir uns so schnell wie möglich um\r\nIhr Anliegen kümmern. Bitte haben Sie dafür Verständnis, dass die\r\nBearbeitung bis zu 10 Werktage dauern kann. Wir bedanken uns für Ihre\r\nentgegengebrachte Geduld und hoffen auf Ihr Verständnis. Mit\r\nfreundlichen Grüßen Ihr DHL Kundenservice\r\n\r\nAllerdings keine Ticketnummer, o.ä., weshalb es zielführender ggf.\r\nsein kann, über ein altes Ticket nachzuhaken oder gleich einen neuen\r\nFall zu eröffnen.\r\n\r\nto: IhreAntwort@dhl.de\r\nsubject: Antrag auf Erstattung ZP130120223302/12345, Sendung\r\nUN029912345JP (DHL 64028612345)\r\n\r\n\r\nSehr geehrte Damen und Herren,\r\nbezugnehmend auf Ticket #0000-123456 möchte ich Ihnen gerne den\r\nkorrigierten Einfuhrabgabenbescheid ZP130120223302/12345 für die Sendung\r\nUN029912345JP (DHL 64028612345) zukommen lassen (s.\r\n“ZP130120223302-12345_korr.pdf” anbei) und um die Erstattung der 6 EUR\r\nAuslagenpauschale bitten.\r\nMeine Bankdaten lauten wie folgt:\r\nGeldinstitut: Hausbank\r\nIBAN: DE49112233445566\r\nBIC: HBKDEXXX\r\nFür Rückfragen und weitere Informationen stehe ich Ihnen jederzeit\r\nzur Verfügung.\r\nMit freundlichen Grüßen\r\n[Empfänger]\r\nAnlagen:\r\n> Paketinfo (Sendung - 64028612345 - UN029912345JP - Paket.pdf)\r\n> korrigierter Abgabenbescheid (korr - ZP130120223302-12345 -\r\n64028612345 - UN029912345JP.pdf)\r\n> Customs Declaration CN22 (Customs_Declaration_CN22.jpg)\r\n\r\n“Excellence. Simply delivered.” Bis zum nächsten Mal…\r\n¯\\(ツ)/¯\r\n\r\nvgl. “Neuerungen ab 01. Juli 2021”\r\nhier: https://www.deutschepost.de/de/b/briefe-ins-ausland/zollinformation/neuerungen-2021.html↩︎\r\nhttps://www.zoll.de/DE/Privatpersonen/Postsendungen-Internetbestellungen/Sendungen-aus-einem-Nicht-EU-Staat/Zoll-und-Steuern/Geschenksendungen/geschenksendungen_node.html#doc289556bodyText3↩︎\r\nhttps://www.zoll.de/DE/Privatpersonen/Postsendungen-Internetbestellungen/Sendungen-aus-einem-Nicht-EU-Staat/Zoll-und-Steuern/Geschenksendungen/geschenksendungen_node.html#doc289556bodyText1↩︎\r\nhttps://www.siemens-logistics.com/de/news/pressemitteilungen/siemens-erhaelt-auftrag-der-deutschen-post-fuer-internationales-drehkreuz↩︎\r\nBitte laden Sie hier Fotos der\r\nkompletten Sendung hoch, auf denen Folgendes zu erkennen ist:\r\n* Alle 6 Seiten des Pakets\r\n* Alle aufgebrachten Label (gut lesbar)\r\n* Die vollständige Paketaufschriftseite mit Rücksendegrund (bitte keine\r\nTeilausschnitte) ↩︎\r\nwarum wohl!?↩︎\r\n",
    "preview": {},
    "last_modified": "2022-09-01T10:54:13+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-08-15-z3950-for-dummies/",
    "title": "Z39.50 for Dummies",
    "description": "This (cleaning up the vaults) is a shameless re-post of a now no longer available blogpost by Wolfram Schneider from Index Data, originally posted in 5 parts 2009/08/27-2010/01/13, which taught me a lot about implementing Z39.50 and the YAZ toolkit. I even printed it out back in the day and it is still worth reading today!",
    "author": [
      {
        "name": "Wolfram Schneider",
        "url": "https://wolfram.schneider.org"
      },
      {
        "name": "Olaf Schmalfuß",
        "url": {}
      }
    ],
    "date": "2020-08-15",
    "categories": [],
    "contents": "\r\n\r\nContents\r\n// Z39.50 for Dummies\r\n// Z39.50 for Dummies\r\nSeries - Part 1\r\n// Z39.50 for Dummies Part\r\n2\r\n// Z39.50 for Dummies\r\nSeries - Part 3\r\n// Z39.50 for Dummies - Part\r\n4\r\n// Z39.50 for Dummies - Part\r\n5\r\n\r\n\r\n// Z39.50 for Dummies\r\nby Wolfram Schneider on 2009/08/27\r\n(http://www.indexdata.com/blog/2009/08/z3950-dummies)\r\nOne of the things Index Data is known for is the YAZ toolkit - an open source\r\nprogrammers’ toolkit supporting the development of Z39.50/SRW/SRU\r\nclients and servers. The first release was in 1995 and I’ve been using\r\nit for my own metasearch engine ZACK Gateway since\r\n1998, long before I joined Index Data.\r\nZ39.50 is a client-server protocol for searching and retrieving\r\ninformation from remote computer databases. It is a mature low level\r\nprotocol like HTTP and FTP. You don’t implement Z39.50 yourself, you use\r\nthe YAZ utilities and the libraries and frameworks for in other\r\nlanguages (C++, PHP, Perl, etc.).\r\nThere are many people who thinks that Z39.50 is a dead standard, and\r\nhard to understand. That is not true. Z39.50 is still growing in use,\r\nstable and very fast. It is the only widely available protocol for\r\nmetasearch.\r\nUsing Z39.50 is not harder than using FTP. I think that the overhead\r\nfor learning Z39.50 is less than a half day for an experienced\r\nprogrammer. Every problem which you have later is not related to the\r\nZ39.50 protocol itself, it is related to underlying system behind the\r\nZ39.50 server. Keep in mind that Z39.50 is an API to access\r\n(bibliographic) databases. It does not define how the data is structured\r\nand indexed in the database.\r\n// Z39.50 for Dummies Series\r\n- Part 1\r\nI will now start a Z39.50 for Dummies series and show some example\r\nhow to access a remote database.\r\nI’m using in the following demos the zoomsh program\r\nfrom the YAZ toolkit\r\nLet’s start with a simple question: does the Library of Congress have\r\nthe book “library mashups”? (I strongly recommend you buy this book - I\r\nwrote chapter 19):\r\n$ zoomsh \"connect z3950.loc.gov:7090/voyager\" 'search \"library mashups\"' quit\r\n\r\nz3950.loc.gov:7090/voyager: 2 hits\r\nThat’s all! Only one line on the command line. A SRU or SOAP request\r\nwould not be shorter.\r\nNow, retrieve the record:\r\n$ zoomsh \"connect z3950.loc.gov:7090/voyager\" 'search \"library mashups\"' \"show 0 1\" \"quit\"\r\n\r\nz3950.loc.gov:7090/voyager: 2 hits\r\n0 database=VOYAGER syntax=USmarc schema=unknown\r\n02438cam 22003018a 4500\r\n001 15804854\r\n005 20090710141909.0\r\n008 090706s2009 nju b 001 0 eng\r\n906 $a 7 $b cbc $c orignew $d 1 $e ecip $f 20 $g y-gencatlg\r\n925 0 $a acquire $b 2 shelf copies $x policy default\r\n955 $b rg11 2009-07-06 $i rg11 2009-07-06 $a rg11 2009-07-08 to Policy (CLED/SHED)\r\n $a td04 2009-07-09 to Dewey $w rd14 2009-07-10\r\n010 $a 2009025999\r\n020 $a 9781573873727\r\n040 $a DLC $c DLC\r\n050 00 $a Z674.75.W67 $b L52 2009\r\n082 00 $a 020.285/4678 $2 22\r\n245 00 $a Library mashups : $b exploring new ways to deliver library\r\ndata / $c edited by Nicole C. Engard.\r\n260 $a Medford, N.J. : $b Information Today, Inc., $c c2009.\r\n263 $a 0908\r\n300 $a p. cm.\r\n504 $a Includes bibliographical references and index.\r\n505 0 $a What is a mashup? / Darlene Fichter -- Behind the scenes : some technical details on mashups / Bonaria Biancu -- Making your data available to be mashed up / Ross Singer -- Mashing up with librarian knowledge / Thomas Brevik -- Information in context / Brian Herzog -- Mashing up the library website / Lichen Rancourt -- Piping out library data / Nicole C. Engard -- Mashups @ Libraries interact / Corey Wallis -- Library catalog mashup : using Blacklight to expose collections / Bess Sadler, Joseph Gilbert, and Matt Mitchell -- Breaking into the OPAC / Tim Spalding -- Mashing up open data with biblios.net Web services / Joshua Ferraro -- SOPAC 2.0 : the thrashable, mashable catalog / John Blyberg -- Mashups with the WorldCat Affiliate Services / Karen A. Coombs -- Flickr and digital image collections / Mark Dahl and Jeremy McWilliams -- Blip.tv and digital video collections in the library / Jason A. Clark -- Where's the nearest computer lab? : mapping up campus / Derik A. Badman -- The repository mashup map / Stuart Lewis -- The LibraryThing API and libraries / Robin Hastings -- ZACK bookmaps / Wolfram Schneider -- Federated database search mashup / Stephen Hedges, Laura Solomon, and Karl Jendretzky -- Electronic dissertation mashups using SRU / Michael C. Witt.\r\n650 0 $a Mashups (World Wide Web) $x Library applications.\r\n650 0 $a Libraries and the Internet.\r\n650 0 $a Library Web sites $x Design.\r\n650 0 $a Web site development.\r\n700 1 $a Engard, Nicole C., $d 1979-\r\n963 $a Amy Reeve; phone: 609-654-6266; email: areeve @ infotoday.com; bc: nellor @ infotoday.com\r\nThe default exchange format for bibliographic records in Z39.50 is\r\nMARC21. This is maybe not what you want to parse yourself.\r\nOk, now let’s download the record in XML format:\r\n$ zoomsh \"connect z3950.loc.gov:7090/voyager\" 'search \"library mashups\"' \"show 0 1 xml\" \"quit\"\r\n\r\nz3950.loc.gov:7090/voyager: 2 hits\r\n0 database=VOYAGER syntax=USmarc schema=unknown\r\n<record xmlns=\"http://www.loc.gov/MARC21/slim\">\r\n <leader>02438cam a22003018a 4500<\/leader>\r\n <controlfield tag=\"001\">15804854<\/controlfield>\r\n <controlfield tag=\"005\">20090710141909.0<\/controlfield>\r\n <controlfield tag=\"008\">090706s2009 nju b 001 0 eng <\/controlfield>\r\n <datafield tag=\"906\" ind1=\" \" ind2=\" \">\r\n <subfield code=\"a\">7<\/subfield>\r\n <subfield code=\"b\">cbc<\/subfield>\r\n <subfield code=\"c\">orignew<\/subfield>\r\n <subfield code=\"d\">1<\/subfield>\r\n <subfield code=\"e\">ecip<\/subfield>\r\n <subfield code=\"f\">20<\/subfield>\r\n <subfield code=\"g\">y-gencatlg<\/subfield>\r\n <\/datafield>\r\n\r\n[large XML output...]\r\n<\/record>\r\nYou can parse the XML output with your favorite tools, usually an\r\nXSLT style sheet.\r\nNext time I will show you how to run a meta search in one line.\r\n-Wolfram\r\nUPDATE: The latest release of YAZ, inspired by this\r\nblog post, supports client-side mapping of MARC to MARCXML, so you can\r\ndump XML records even from targets that do not support XML.\r\n// Z39.50 for Dummies Part 2\r\nby Wolfram Schneider on 2009/08/31\r\n(http://www.indexdata.com/blog/2009/08/z3950-dummies-part-2)\r\nIn the last blog post Z39.50\r\nfor Dummies I gave an introduction on how to use the zoomsh program\r\nto access the Z39.50 Server of the Library of Congress.\r\nToday I will show you how to run a simple metasearch on the command\r\nline. You want to know which library has the book with the ISBN\r\n0-13-949876-1 (UNIX network programming / W. Richard Stevens)? You can\r\nrun the zoomsh in a shell loop.\r\nPut the list of databases (zURL’s) line by line in the text file\r\nzurl.txt:\r\nz3950.loc.gov:7090/voyager\r\nmelvyl.cdlib.org:210/CDL90\r\nlibrary.ox.ac.uk:210/ADVANCE\r\nz3950.library.wisc.edu:210/madison\r\nand run a little loop in a shell script:\r\n$ for zurl in `cat zurl.txt`\r\ndo\r\n zoomsh \"connect $zurl\" \\\r\n \"search @attr 1=7 0-13-949876-1\" \"quit\"\r\ndone\r\n\r\n\r\nz3950.loc.gov:7090/voyager: 0 hits\r\nmelvyl.cdlib.org:210/CDL90: 1 hits\r\nlibrary.ox.ac.uk:210/ADVANCE: 1 hits\r\nz3950.library.wisc.edu:210/madison: 0 hits \r\nOf course it takes time to run one search request after another. How\r\nabout a parallel search? Modern xargs(1) commands on BSD based Operating\r\nSystems (MacOS, FreeBSD) and the GNU xargs supports to run several\r\nprocesses at a time.\r\nThis example runs up to 2 search request at a time and is 2 times\r\nfaster than the shell script above:\r\n$ xargs -n1 -P2 perl -e 'exec \"zoomsh\", \"connect $ARGV[0]\", \"search \\@attr 1=7 0-13-949876-1\", \"quit\"' &lt; zurl.txt\r\n\r\nmelvyl.cdlib.org:210/CDL90: 1 hits\r\nlibrary.ox.ac.uk:210/ADVANCE: 1 hits\r\nz3950.loc.gov:7090/voyager: 0 hits\r\nz3950.library.wisc.edu:210/madison: 0 hits\r\nYou see here that the order of responses is different, the fastest\r\ndatabases wins and displayed first.\r\nI think it is safe to run up to 20 searches in parallel on modern\r\nhardware. Note that there is a lot of process overhead here, for each\r\nrequest 2 processes will be executed. If a connection hangs you must\r\nwait until you hit the time out.\r\nThis was an example how easy it is to run your own metasearch on the\r\ncommand line. If you want setup a real metasearch for your organization\r\nI recommend to try out our metasearch middleware pazpar2, featuring merging,\r\nrelevance ranking, record sorting, and faceted results. In a nutshell,\r\npazpar2 is a web-oriented Z39.50 client. It will search a lot of targets\r\nin parallel and provide on-the-fly integration of the results. The\r\ninterface is entirely webservice-based, and you can use it from any\r\ndevelopment environment. The pazpar2 home page is http://www.indexdata.com/pazpar2\r\n// Z39.50 for Dummies Series\r\n- Part 3\r\nby Wolfram Schneider on 2009/09/09\r\n(http://www.indexdata.com/blog/2009/09/z3950-dummies-series-part-3)\r\nThis is part 3 of the Z39.50 series for dummies. In the first\r\npart I explained what Z39.50 is and how to run a simple search. In\r\nthe second\r\npart I showed how to run a simple meta search on the command\r\nline.\r\nI searched for the book: UNIX network programming / W. Richard\r\nStevens, ISBN 0-13-949876-1 in four large libraries:\r\n$ for zurl in `cat zurl.txt`\r\ndo\r\n zoomsh \"connect $zurl\" \\\r\n \"search @attr 1=7 0-13-949876-1\" \"quit\"\r\ndone\r\n\r\nz3950.loc.gov:7090/voyager: 0 hits\r\nmelvyl.cdlib.org:210/CDL90: 1 hits\r\nlibrary.ox.ac.uk:210/ADVANCE: 1 hits\r\nz3950.library.wisc.edu:210/madison: 0 hits\r\nOnly 2 out of 4 libraries own this must-have book. Can this be true?\r\nWell, lets modify the ISBN and search without dashes (‘-’)\r\n$ for zurl in `cat zurl.txt`\r\ndo\r\n zoomsh \"connect $zurl\" \\\r\n \"search @attr 1=7 0139498761\" \"quit\"\r\ndone\r\n\r\nz3950.loc.gov:7090/voyager: 1 hits\r\nmelvyl.cdlib.org:210/CDL90: 1 hits\r\nlibrary.ox.ac.uk:210/ADVANCE: 1 hits\r\nz3950.library.wisc.edu:210/madison: 1 hits\r\nBingo - every library has a copy of UNIX network programming by W.\r\nRichard Stevens!\r\nZ39.50 defines the syntax to search in a database. It does not define\r\nthe semantic of a search, how an ISBN is structured.\r\nIf you build a search engine on top of Z39.50 you need an additional\r\nlayer to handle the semantic of a search for each database. (You need\r\nthis layer too to add workaround for broken implementations)\r\nIn this example above we must remove the dashes in an ISBN search for\r\nthe Library of Congress and University of Wisconsin-Madinson\r\nLibraries.\r\nAnother thing which you must be aware: libraries use for historical\r\nreasons different character sets: utf-8, iso8859-1, iso5426 and marc8.\r\nYou must convert your search query to the right character set for each\r\nlibrary, for searching and retrieving the records.\r\nIn this article I described the challenges to run a meta search on\r\ntop of Z39.50. All these problems are due the underlying databases and\r\nnot Z39.50 - you will have the same problems if you use a web based XML\r\nservices such as SRU or a proprietary, vendor-based API. The truth is\r\nthat running a metasearch is not a trivial task.\r\n// Z39.50 for Dummies - Part 4\r\nby Wolfram Schneider on 2009/10/12\r\n(http://www.indexdata.com/blog/2009/10/z3950-dummies-part-4)\r\nThis is part 4 of the series Z39.50\r\nfor dummies.\r\nLibraries store and exchange bibliographic data in MARC records. A\r\nMARC record is a MAchine-Readable\r\nCataloging record. It was developed at the Library of Congress (LoC) beginning\r\nin the 1960s.\r\nA dump of the LoC catalog (and other libraries) is available at the\r\nInternet Archive in the collection marcrecords. The\r\nLoC\r\ncatalog dump is split into 29 files, part01.dat to part29.dat. Each\r\nfile is roughly 200MB large.\r\nThe great news is that the data from LoC is public domain (already\r\npaid by the US taxpayers, thank you!) and you can use the data for your\r\nown system.\r\nBefore you can import data, you must validate, convert, or fix the\r\nbibliographic data. I will show now how you can do this with the Index\r\nData YAZ toolkit. The YAZ\r\ntoolkit contains the program yaz-marcdump\r\nto dump MARC records.\r\nyaz-marcdump called without an option will print the records in line\r\nformat:\r\n$ yaz-marcdump part01.dat | more\r\n\r\n00720cam  22002051  4500\r\n001    00000002\r\n003 DLC\r\n005 20040505165105.0\r\n008 800108s1899    ilu           000 0 eng\r\n010    $a    00000002\r\n035    $a (OCoLC)5853149\r\n040    $a DLC $c DSI $d DLC\r\n050 00 $a RX671 $b .A92\r\n100 1  $a Aurand, Samuel Herbert, $d 1854-\r\n245 10 $a Botanical materia medica and pharmacology; $b drugs considered from a botanical, pharmaceutical, physiological, therapeutical and toxicological standpoint. $c By S. H. Aurand.\r\n260    $a Chicago, $b P. H. Mallen Company, $c 1899.\r\n300    $a 406 p. $c 24 cm.\r\n500    $a Homeopathic formulae.\r\n650  0 $a Botany, Medical.\r\n650  0 $a Homeopathy $x Materia medica and therapeutics.\r\n[...]\r\nFirst converts the MARC21 records in MARC-8 encoding to MARC21 in\r\nUTF-8 encoding:\r\n$ yaz-marcdump -f marc-8 -t utf-8 -o marc \\\r\n       part01.dat > part.mrc\r\nFor MARC21, the leader offset 9 tells whether it is really MARC8\r\n(almost always the case) or whether it’s UTF-8. A MARC21 must have\r\nposition 9=‘a’ (value 97). For this reason, the option -l for\r\nyaz-marcdump may come in handy:\r\n$ yaz-marcdump -f marc-8 -t utf-8 -o marc \\\r\n       -l 9=97 part01.dat > part.mrc\r\nIf you prefer MARCXML instead MARC21 records you may convert the\r\nrecords:\r\n$ yaz-marcdump -o marcxml -f MARC-8 -t UTF-8 \\\r\n    part01.dat > part.marcxml\r\n\r\n<collection xmlns=\"http://www.loc.gov/MARC21/slim\">\r\n<record>\r\n  <leader>00720cam a22002051  4500<\/leader>\r\n  <controlfield tag=\"001\">   00000002 <\/controlfield>\r\n  <controlfield tag=\"003\">DLC<\/controlfield>\r\n  <controlfield tag=\"005\">20040505165105.0<\/controlfield>\r\n  <controlfield tag=\"008\">800108s1899    ilu           000 0 eng\r\n<\/controlfield>\r\n  <datafield tag=\"010\" ind1=\" \" ind2=\" \">\r\n    <subfield code=\"a\">   00000002 <\/subfield>\r\n  <\/datafield>\r\n  <datafield tag=\"035\" ind1=\" \" ind2=\" \">\r\n    <subfield code=\"a\">(OCoLC)5853149<\/subfield>\r\n  <\/datafield>\r\n[...]\r\nThe Library of Congress has over 7 million records. That’s huge data,\r\ntotal 5.6GB raw data. If you compress that data it is only 1.7GB.\r\nTo convert compressed data, run yaz-marcdump in a UNIX pipe:\r\n$ zcat part01.dat.gz | yaz-marcdump -f MARC-8 \\\r\n  -t UTF-8 -o marcxml /dev/stdin > part01.marcxml \r\nYou can search a marc dump with the UNIX grep tool:\r\n$ yaz-marcdump -f marc-8 -t utf-8 part01.dat | \\\r\n      grep Sausalito\r\n\r\n260    $a Sausalito, Calif. : $b University Science Books, $c 2000.\r\n260    $a Sausalito, Calif. : $b Math Solutions Publications, $c c2000.\r\n260    $a Sausalito, Calif. : $b Post-Apollo Press, $c c2000.\r\n260    $a Sausalito, Calif. : $b University Science Books, $c c2002.\r\n260    $a Sausalito, Calif. : $b Post-Apollo Press, $c c2000.\r\n260    $a Sausalito, CA : $b Toland Communications, $c c2000.\r\n260    $a Sausalito, CA : $b In Between Books, $c 2001.\r\n[...]\r\nThe yaz-marcdump\r\ntool supports the character sets UTF-8, MARC-8, ISO8859-1, ISO5426 and\r\nsome other encodings. For more information, see the yaz-iconv\r\nmanual pages.\r\nIn this article I showed how to validate, convert, or fix\r\nbibliographic data dumped in MARC format. Next time I will show some\r\nadvanced examples how to analyze MARC records on modern standard PC\r\nhardware.\r\n// Z39.50 for Dummies - Part 5\r\nby Wolfram Schneider on 2010/01/13\r\n(http://www.indexdata.com/blog/2010/01/z3950-dummies-part-5)\r\nThis is part 5 of the series Z39.50\r\nfor dummies. In the 4th\r\npart I showed how to run convert MARC21 records to line format or\r\nXML.\r\nIn this article I will show you how to analyze MARC data on a modern\r\nPC hardware. PC are very fast now and incredibly cheap. You can rent a\r\nquad-core Intel machine with 8GB RAM and unlimited traffic for 40\r\nEuro/month (+VAT) in a data center.\r\nIf the computer is fast enough, you don’t have to spend too much time\r\non complex algorithms. You can use the raw power of your computer and do\r\na brute force approach.\r\nIn the following example I will use the 7 million records from a dump\r\nof the Library of Congress (LoC) catalog. For details, please read the\r\nprevious article Z39.50\r\nfor Dummies - Part 4.\r\n$ for i in *.dat; do\r\n    yaz-marcdump -f marc-8 -t utf-8 -o line\r\n  done > loc.txt\r\n\r\n$ du -hs loc.txt\r\n4.9G\r\nThe line dump of the LoC is 4.9GB large and fits into main memory -\r\ngreat!\r\n# count for the last name “Calaminus”\r\n$ egrep -c Calaminus loc.txt\r\n4 hits, the search took 4 seconds real time\r\n# count records with <span class=\"caps\">ISBN<\/span> number\r\n$ egrep -c ^020 loc.txt\r\n3999863\r\nThere are nearly 4 million ISBN numbers (out of 7 million records).\r\nThe search took 11 seconds.\r\n# count <span class=\"caps\">URL<\/span>s\r\n$ egrep -c http:// loc.txt\r\n265540\r\nThere are 265,540 URLs in the LoC records.\r\n# check for subject headings for the city of \r\n# Sausalito, California using regular expression\r\n$ egrep -c ‘^[67][0-<span class=\"caps\">9[0<\/span>-9].*Sausalito’ loc.txt\r\n19\r\nThere are 19 subject headings for Sausalito\r\n# search with a typo in name (a => o)\r\n$ egrep Sausolito loc.txt\r\nNo hits due a typo in the name, try it with agrep, a grep program with\r\napproximate matching capabilities:\r\n$ agrep -c -1 Sausolito loc.txt\r\n282\r\n282 hits, the search took 8 seconds\r\nThe examples above are for software developers and experienced\r\nlibrarians. They are helpful for a quick check of your bibliographic\r\nrecords, for data mining, analyzing or to double-check if your indexer\r\nworks correctly.\r\nIf you want setup a public system for end-users you need of course a\r\nreal full text engine as our zebra software.\r\nRead the other articles of the series Z39.50 for Dummies: Part\r\nI, Part\r\nII, Part\r\nIII, Part\r\nIV, Part\r\nV\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-06-03T13:23:01+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-07-07-wintaskschedulerforcygwin/",
    "title": "Win Task Scheduler for cygwin",
    "description": "HowTo run bash scripts via Cygwin as Scheduled Task (i.e. \"Aufgabenplanung\") in Windows",
    "author": [
      {
        "name": "Olaf Schmalfuß",
        "url": {}
      }
    ],
    "date": "2020-07-07",
    "categories": [],
    "contents": "\r\nHowTo\r\nrun bash script via Cygwin as Scheduled Task (i.e. “Aufgabenplanung”) in\r\nWindows\r\nFor process automation it is not necessary to rely on Outlook\r\nreminders if you’re on Windows and cannot (or want not) run Cron as a\r\nservice to schedule scripts or programs:\r\nWindows brings its native service called “Scheduled Tasks” (or\r\n“Aufgabenplanung” in the German version)\r\n// Open “Scheduled Tasks” App\r\nhit Windows Key + R to open command prompt, enter\r\ntaskschd.msc, which works regardless of Win version/\r\nlanguage\r\nalternatively\r\nsearch APP for either “Scheduled Task” or\r\n“Aufgabenplanung”\r\nor locate the “EXE” in\r\nC:\\Windows\\System32\\taskschd.msc\r\n…\r\n\r\n// Set up a new Job\r\nRight-click on an empty space in the Tasks’ overview and select “new\r\ntask” (i.e. “Einfache Aufgabe erstellen”).\r\nIn “Actions” (i.e. “Aktionen”) enter the path to cygwin in the\r\n“Program/script” field, e.g.C:\\cygwin64\\bin\\bash.exe\r\nIn the “Add arguments” box, enter “-l -c” and the full path to the\r\nbash command to run surrounded by quotes in UNIX notation,\r\ni.e. with forward slashes and as seen from within cygwin, and\r\nnot simply the Windows path, e.g.-l -c \"/tmp/pdw_test/pull_chapter_report_from_jobscheduler.sh\"\r\n -l: Run as if logged on at a shell\r\n -c: Run this command\r\nIn the “Start in” field enterC:\\cygwin64\\bin\r\nAdd a description and give your task a meaningful name, so that when\r\nyou come back to it in a year you know what it’s for and you’re\r\ndone!\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-07-07-wintaskschedulerforcygwin/example.png",
    "last_modified": "2022-09-01T10:53:23+02:00",
    "input_file": {},
    "preview_width": 1258,
    "preview_height": 839
  },
  {
    "path": "posts/2018-05-05-download-youtube-using-only-vlc/",
    "title": "download YouTube (videos | mp3) without browser extension using only VLC",
    "description": "There is a multitude of browser add-ons and extensions available for downloading YouTube videos or converting to mp3 that are usually somewhat obscure, but there is actually nothing more needed than the popular VLC player. Here is how!",
    "author": [
      {
        "name": "Olaf Schmalfuss",
        "url": "https://www.datamercs.net"
      }
    ],
    "date": "2018-05-05",
    "categories": [],
    "contents": "\r\n\r\nContents\r\n//\r\nIntroduction\r\n// Download originial\r\nYouTube video with VLC\r\n// Download audio from\r\nYouTube with VLC\r\n// Download video |\r\naudio from m3u with VLC\r\n\r\n// Introduction\r\nIn our HowTo\r\ndownload “Mediathek” features for archival purposes we looked into\r\nhow to download features from your typical German public broadcasting\r\nMediathek.\r\nWe might want a similar solution to YouTube for having media\r\navailable offline, e.g. for live presentations when internet service\r\ncannot be guaranteed.\r\nRegrettably the same trick is not working for YouTube. Naturally, there\r\nis a multitude of browser add-ons and extensions available for\r\ndownloading YouTube videos that might range from deprecated to outright\r\nmalicious, but are usually somewhat obscure.\r\nBut, as of writing this, there is actually nothing more needed than\r\nthe popular VLC player↗.\r\nVLC is a feature packed, free and open source cross-platform multimedia\r\nplayer and framework that plays virtually all multimedia files as well\r\nas DVDs, Audio CDs, VCDs, and various streaming protocols.\r\nDownload here, as this is what we will use: https://www.videolan.org\r\n// Download originial\r\nYouTube video with VLC\r\nVLC does not only play music or videos from disk, but is also a\r\ncomplete software solution for video streaming that can be used to\r\ncapture and transcode media streams, such as YouTube videos!\r\nFor information, also on the command line options see the documentation\r\nhere: https://wiki.videolan.org/Documentation:Streaming_HowTo_New/\r\nFor the most straight forward approach we will go as follows:\r\nopen YouTube video and copy the\r\nURL\r\nin VLC hit Ctrl + N\r\nalternatively: Media >> Open Network Stream (Ctrl\r\n+ N)\r\npaste copied link as network URL into VLC\r\n\r\n\r\n\r\nhit Play\r\nAchievement Unlocked: even though not part of this\r\ntutorial, we already unlocked the option of watching YouTube videos\r\noutside of the browser - without advertisements or YT recommendations\r\nfor a safe presentation mode\r\nthen Ctrl+J to view the Current Media\r\nInformation\r\nalternatively: Tools >> Codec\r\nInformation\r\nCopy the URL given under Location\r\n\r\n\r\n\r\npaste the copied Codec-URL back the browser\r\nright click on video and save\r\nalternatively: download using curl\r\n\r\n$ curl \"your_copied_Codec-URL_here\" -o YT.mp4\r\n\r\n// Download audio from\r\nYouTube with VLC\r\nSimilar to the above, we can use the Codec-URL to download only the\r\nAudio of the video, though not by niftily extracting the\r\noriginal audio stream, but simply by transcoding↗, which\r\nmeans some quality loss of course.\r\nFor downloading a MP3 off YouTube follow the steps 1-6 from above,\r\nthen instead of pasting the Codec-URL back into the browser, paste the\r\nlink back into VLC and instead of play we choose to\r\nConvert:\r\nOpen YouTube video and copy the URL\r\nin VLC hit Ctrl + N\r\npaste copied link as network URL into VLC\r\nhit Play\r\nthen Ctrl+J to view the Current Media\r\nInformation\r\nCopy the URL given under Location\r\ntype Ctrl + N again\r\npaste the copied Codec-URL back under network\r\nURL\r\ninstead of hitting Play select Convert\r\nfrom the drop down:\r\n\r\n\r\n\r\na new Window opens with the Codec-URL as Source\r\nselect Convert as Audio - MP3\r\nyou might want to change the conversion setting, like code or\r\nbitrate, via Edit selected profile first\r\nchoose a destination for the converted MP3\r\npress Start\r\n\r\n\r\n\r\npress Play again to really start the\r\nconversion\r\n\r\n\r\n\r\nVLC will do the rest and show you the status in the progress\r\nbar\r\n\r\n\r\n\r\n// Download video |\r\naudio from m3u with VLC\r\nThe same procedures can be used to play, download or convert\r\nM3U streams directly in VLC.\r\nM3U files are basically a type of playlist, where the location of each\r\nitem on the playlist is placed on a new line, and which are widely used\r\nby (German) public broadcasting for streams, instead the of un-split mp3\r\nor mp4 files we could find otherwise in the Mediathek.\r\nFor that, simply download the m3u TXT file to local or paste the m3u’s\r\nlink into the network URL again (via Ctrl +\r\nN) and follow the steps above.\r\n\r\n\r\n\r\n",
    "preview": "posts/2018-05-05-download-youtube-using-only-vlc/distill-preview.png",
    "last_modified": "2022-09-01T10:52:50+02:00",
    "input_file": {},
    "preview_width": 856,
    "preview_height": 673
  },
  {
    "path": "posts/2018-04-05-rbbkultur-mediathek-downloader/",
    "title": "rbbKultur - Mediathek-Downloader",
    "description": "HowTo download \"Mediathek\" features for archival purposes",
    "author": [
      {
        "name": "Olaf Schmalfuß",
        "url": {}
      }
    ],
    "date": "2018-04-05",
    "categories": [],
    "contents": "\r\nrbbKultur -\r\nMediathek-Downloader\r\n// Introduction\r\nFor archival or educational purposes it might be useful to know how\r\nto download content from public radio and TV broadcasters’ archives,\r\ni.e. “Mediathek”.\r\nThis only works, when the program is archived as a whole file in a\r\nconvenient format like mp3 or mp4, and not in a chunked m3u streaming\r\nformat. This we will see in step 4. below. So if we encounter m3u, we\r\nwill use VLC instead to capture and convert the stream, see:link to VLC howto\r\n1. select Program\r\nrbbKultur - Mediathek\r\n\r\nhttps://www.kulturradio.de/zum_nachhoeren/\r\n\r\nchose date and program\r\n2. inspect element\r\ni.e. launch Developer Tools\r\nFireFox: CTRL+SHIFT+C\r\nChrome: CTRL+SHIFT+I\r\n\r\n3. Start Mediathek player\r\ni.e. start the podcast or video once we have the Developer\r\nTools open, so that we can analyze the traffic under\r\nNetwork\r\n4. back in Developer Tools\r\nunder Network right click on the File (media/\r\nmpeg)\r\nthen Copy URL or copy as cURL\r\n\r\n\r\n\r\n5. Download via bash script\r\nadjust script\r\n\r\nrbb_downloader()\r\n  {\r\n    if [ -z \"$1\" ]\r\n      then echo \"Dateiname fehlt\"\r\n    elif [ -z \"$2\" ]\r\n      then  \"Download URL fehlt\"\r\n    else\r\n      DWNDIR=/tmp/Downloads/AlteMusik\r\n      AMDOWN=$1\r\n      rbbmediapmdp=$2\r\n      curl $rbbmediapmdp -o $DWNDIR/\"$AMDOWN\"\r\n      ls -htl $DWNDIR\r\n  fi\r\n  }\r\n  \r\n# usage:\r\nrbb_downloader \"File_Name\" \"Download_URL\"\r\n\r\noptional: save in bashrc\r\n\r\nC:\\cygwin64\\home\\schmalfuss\\.bashrc\r\n\r\nrun\r\ne.g. \r\n\r\n\r\nrbb_downloader \"Alte Musik - 2020-07-08 - Heinrich Isaac - Lieder und Motetten.mp3\" \"https://rbbmediapmdp-a.akamaihd.net/content/74/da/74da3627-5312-4f34-b681-93c48f12202d/0a858af8-b788-430b-9f5a-b5c678701a18_53d97702-265a-4586-a6d8-c8c7ad9bac80.mp3\"\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2018-04-05-rbbkultur-mediathek-downloader/dwnfunction.png",
    "last_modified": "2022-09-01T10:51:58+02:00",
    "input_file": {},
    "preview_width": 432,
    "preview_height": 274
  },
  {
    "path": "posts/2017-11-09-bash-test-command/",
    "title": "bash 'test' command",
    "description": "A mental note and short reminder on how to use the \"test\" command in bash if-loops or using the && and || operators for conditional scripting.",
    "author": [
      {
        "name": "Olaf Schmalfuß",
        "url": "https://www.datamercs.net"
      }
    ],
    "date": "2017-11-09",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nUsing ‘test’ in loops / conditional statements\r\nif-else\r\n&& and || operators\r\n\r\nUsing ‘test’ for checking files - test existence\r\nUsing ‘test’ for checking variables\r\nfurther reading\r\n\r\nIntroduction\r\nThis is a short introduction - and personal reminder - on how to use the test command.\r\nThe Google “Shell Style Guide”↗ prefers double [[ … ]] over the older (but POSIX portable) single [ … ] or literal test and I won’t question this. 1\r\nThe test command returns 0 (True) or 1 (False), which we can use conditionally in if-loops or using the && and || operators, and one could also simply examine the return value by displaying $? e.g.\r\n\r\n$ [[ \"foo\" != \"bar\" ]]; echo $?\r\n0\r\n\r\n$ [[ \"foo\" == \"bar\" ]]; echo $?\r\n1\r\n\r\n$ [[ \"foo\" == \"foo\" ]]; echo $?\r\n0\r\n\r\n$ [[ \"foo\" != \"foo\" ]]; echo $?\r\n1\r\n\r\n$ [[ 5 > 2 ]]; echo $?\r\n0\r\n\r\nThere are tons of applications, but we will mostly use it to grep for certain outputs or check variables or for the existence of files.\r\nConsult help test for further details.\r\nUsing ‘test’ in loops / conditional statements\r\nif-else\r\n\r\n\r\nvar=baz # this is our test input, i.e. [[ \"foo\" == $var ]] will return \"False\" (i.e. \"1\"): [[ \"foo\" == $var ]]; echo $? vs. [[ \"baz\" == $var ]]; echo $?\r\n\r\nif [[ \"foo\" == $var ]]; then\r\n  echo \"VAR equals foo\"\r\nelif [[ \"bar\" == $var ]]; then\r\n  echo \"VAR equals bar\"\r\nelse\r\n  echo \"we got it all wrong and VAR equals something else\"\r\nfi\r\n\r\nThe use of echo here is just a substitute for any other command that might be used here.\r\n&& and || operators\r\nThere is a more cunning, and cryptic or 31337, way of writing if-statements using the && and || operators.&& and || are actually LOGICAL operators, i.e.\r\n\r\n&& = AND\r\n|| = OR\r\n\r\nthe general logic for scripting or chaining commands would be\r\n\r\n\"FOO ; BAR\"      # Run FOO and then BAR, regardless of success of FOO\r\n\"FOO && BAR\"     # Run BAR if FOO succeeded, i.e. returns \"True\"\r\n\"FOO || BAR\"     # Run BAR if FOO failed, i.e. returns \"False\"\r\n\"FOO &\"          # Run FOO in the background.\r\n\"FOO | BAR\"      # pipe the output of FOO into BAR for further processing\r\n\r\nto substitute an if-else-loop with && and || use as follows\r\n\r\n# the nifty way\r\n[[ \"foo\" != \"bar\" ]] && echo \"true, foo != bar\" || echo \"if false, obviously VAR must have been foo then as well\"\r\n\r\n# which is the same as\r\nif [[ \"foo\" != \"bar\" ]]; then\r\n  echo \"true, foo != bar\"\r\nelse\r\n  echo \"if false, obviously VAR must have been foo then as well\"\r\nfi\r\n\r\nthese operators can also be used to chain multiple tests/ conditions together into one test e.g.\r\n\r\n$ var=baz\r\n# OR; is one or the other \"True\"\r\n$ [[ \"foo\" == $var || \"baz\" == $var ]] && echo \"true\" || echo \"false\"\r\ntrue\r\n# AND; both must be \"True\"\r\n$ [[ \"foo\" == $var && \"baz\" == $var ]] && echo \"true\" || echo \"false\"\r\nfalse\r\n\r\nUsing ‘test’ for checking files - test existence\r\nFor example, when I’m downloading, extracting and forwarding files based on a list of ISBNs and do not know whether I’ll find a package for each of the given ISBNs, then I need to know the failed ISBN, so that I do not include it within my extra metadata file, in which I only want to provide data for succeeded ISBNs. Obviously, I’m aware that I could include a test at an earlier stage already, but this is my workshop;-)\r\n\r\nISBNLIST=ISBNs4NYU.txt\r\nBASEDIR=$(pwd)\r\nWORKDIR=$BASEDIR/ProQuest_Deposit_$(date +%F); mkdir $WORKDIR; cp $ISBNLIST $WORKDIR; cd $WORKDIR\r\n\r\n# Download WebPDFs directly out of the PDW\r\nfor ISBN in $(cat $ISBNLIST); do\r\n  echo $ISBN\r\n  # getting \"documentId\"\r\n  DOCUMENTID=$(curl -s --proxy \"http://proxy-URL:1337\" https://product-data-warehouse-URL/publication/$ISBN | grep '\"id\"' | head -n1 | grep -oP '(?<=\"id\" : \").+?(?=\")')\r\n  # downloading ZIP\r\n  curl -s --proxy \"http://proxy-URL:1337\" https://product-data-warehouse-URL/document/$DOCUMENTID/zip -o $ISBN.zip\r\n  # unzip WebPDF only\r\n  unzip -q -j $ISBN.zip *$DOCUMENTID.pdf\r\n  # rename to ISBN.pdf\r\n  mv $DOCUMENTID.pdf $ISBN.pdf\r\n  # if WebPDF exists, then upload to target FTP\r\n  if [[ -f $ISBN.pdf ]]; then\r\n    curl -T $ISBN.pdf -u UN:PW ftp://ftp.data-recipient.com/upload/\r\n    echo \"$ISBN\" >> isbn_list_4_onix.txt\r\n    else echo \"$ISBN\" >> WebPDF_missing.txt\r\n  fi\r\n  # clean-up\r\n  rm $ISBN.zip $ISBN.pdf\r\ndone\r\n\r\ncd $BASEDIR\r\n\r\nI usually only need -f, but again check help test for other useful options like\r\n\r\nFile operators:\r\n-d FILE           True if file is a directory.\r\n-e FILE           True if file exists [regardless of type, e.g. file, directory, device etc.].\r\n-f FILE           True if file exists and is a regular file [and not a directory or device etc.].\r\n-s FILE           True if file exists and is not empty.\r\nA -nt B           Test if file A is newer than file B, by modification date.\r\nA -ot B           Test if file A is older than file B.\r\n\r\nThis check could also be negated, so that the outcome is “True” when the file does NOT exist:\r\n\r\nif [[ ! -f $ISBN.pdf ]]; then\r\n    echo \"$ISBN.pdf does not exist.\"\r\nfi\r\n\r\nUsing ‘test’ for checking variables\r\nFor example, I want to prompt the user to specify a file, like an ISBN list, when there was no command line argument (i.e. $1) given in the first place:\r\n\r\n# check for input variable, if empty prompt for file in current directory, else move on with the given input variable\r\nif [[ -z \"$1\" ]]; then\r\n  echo; echo \"Please select ISBN list: (choose number)\"; echo\r\n  select L in *; do test -n \"$L\" && break; echo \">>> Invalid Selection\"; done\r\nelse\r\n  L=$1\r\nfi\r\n\r\n\r\nString operators:\r\n-z STRING           True if string is empty.\r\n-n STRING           True if string is NOT empty.\r\n\r\n\r\n# nifty alternative\r\n[[ -z \"$var\" ]] && echo \"true, VAR is empty\" || echo \"false, VAR is Not empty\"\r\n\r\nfurther reading\r\nThis IBM tutorial on Bash test and comparison functions↗ is also very helpful.  \r\n to be continued …\r\n\r\n [[ … ]] reduces errors as no pathname expansion or word splitting takes place between [[ and ]]. In addition, [[ … ]] allows for regular expression matching, while [ … ] does not. ↩︎\r\n",
    "preview": "posts/2017-11-09-bash-test-command/helptest.png",
    "last_modified": "2022-02-24T16:51:35+01:00",
    "input_file": {},
    "preview_width": 640,
    "preview_height": 320
  },
  {
    "path": "posts/2017-09-11-bash-cat-the-poor-mans-text-editor/",
    "title": "bash cat - the poor man's text editor",
    "description": "Not many people realize that on bash you do not necessarily need a full blown text editor like vim, emacs or nano, or need to remember the heredoc logic to write text on the command line:\nAll you need is some \"cat magic\"\nHere is how!",
    "author": [
      {
        "name": "Olaf Schmalfuß",
        "url": "https://www.datamercs.net"
      }
    ],
    "date": "2017-09-11",
    "categories": [],
    "contents": "\r\n// cat: concatenate and WRITE\r\nfiles\r\nNot many people realize that on bash you do not\r\nnecessarily need a full blown text editor like vim, emacs or nano, or\r\nneed to remember the heredoc logic to write multi-line text on the\r\ncommand line, but that for most cases good ol’ cat\r\nis all you needed.\r\nAll you need to do is cat into a(n empty) file and close\r\nwith CTRL+d\r\nOne can add some extra text with >> again\r\n\r\n$ cat > text.txt\r\nThe Lord of the Rings\r\nis one of those things\r\nif you like it you do\r\n\r\n# CTRL+d to finish\r\n\r\n# let's add some more text to the same file\r\n$ cat >> text.txt\r\nif you don't then you boo\r\n\r\n# CTRL+d to finish\r\n\r\n# result:\r\n$ cat text.txt\r\nThe Lord of the Rings\r\nis one of those things\r\nif you like it you do\r\nif you don't then you boo\r\n\r\n\r\n# same in heredoc logic\r\ncat << EOF > text.txt\r\nThe Lord of the Rings\r\nis one of those things\r\nif you like it you do\r\nif you don't then you boo\r\nEOF\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2017-09-11-bash-cat-the-poor-mans-text-editor/cat_magic.png",
    "last_modified": "2022-09-01T10:50:31+02:00",
    "input_file": {},
    "preview_width": 196,
    "preview_height": 100
  },
  {
    "path": "posts/2017-08-24-howto-batch-rename-on-the-command-line/",
    "title": "Howto batch rename on the command line",
    "description": "Renaming files is one of the everyday tasks, for which many people genuinely scramble and download the most dubious programs from the internet, while one could actually do this pretty easy with command line tools.  \nHere we will put down some quick and easy bash and PowerShell examples that anybody can do!",
    "author": [
      {
        "name": "Olaf Schmalfuß",
        "url": "https://www.datamercs.net"
      }
    ],
    "date": "2017-08-24",
    "categories": [],
    "contents": "\r\n\r\nContents\r\n//\r\nIntroduction\r\n//\r\nrenaming files in bash\r\n// renaming files in\r\nPowerShell\r\n// rename using mapping list\r\n\r\n// Introduction\r\nRenaming files is a quite common everyday task, especially when you\r\nreceive many files in different formats that have to be aligned to your\r\nin-house standards, or when on the other hand you have to send out lots\r\nof files and have to meet the data recipients specification, which\r\ncoincidentally I both have to do quite regularly.\r\nThus, here’re some assorted oneliners for future reference, quick\r\ncopy&pastes or for sharing.\r\n// renaming files in bash\r\nWhile there are certainly dedicated libraries for renaming files,\r\nlike ‘rename’, renaming can be achieved basically with native bash\r\nfunctions only, using cp or mv together with\r\nsed or shell\r\nparameter expansion↗\r\nIn the following example we need to mass rename files from\r\n\r\nepub_dgo_9783110947649.zip to\r\nepub_arv_9783110947649.zip\r\n\r\ni.e. ‘simply’ changing ‘dgo’ to ‘arv’, but this could also be used to\r\nchange or normalize file extensions, like ‘.tiff’ to ‘.tif’ etc.\r\nObviously, use cp instead of mv if you want to\r\nkeep your original files or if unsure whether the\r\n${parameter/pattern/string} replacement will work;-)\r\n\r\n# rename scripts (from 1337 to legible)\r\n\r\n#1#\r\nfor f in *; do mv \"$f\" \"${f/dgo/arv}\"; done\r\n\r\n#1.2 change file extension from '.html' to '.txt'\r\nfor f in *.html; do mv \"$f\" \"${f%.html}.txt\"; done\r\n#1.3 or simply append ext\r\nfor f in *; do mv \"$f\" \"${f}.txt\"; done\r\n\r\n#2#\r\nfor f in * ; do mv $f $(echo $f | sed 's/dgo/arv/'); done\r\n\r\n#3#\r\nfor f in *; do\r\n  fnew=$(sed 's/dgo/arv/' <<< $f)\r\n  mv $f $fnew\r\ndone\r\n\r\n// renaming files in\r\nPowerShell\r\nIf you are on Windows and have no bash options like cygwin\r\nor the Windows Subsystem for Linux then you can easily achieve\r\nthe same result using PowerShell.\r\nGiven the same example from above, i.e. changing ‘dgo’ to ‘arv’, just\r\ntry the following:\r\n\r\n#4# PowerShell rename\r\ndir | rename-item –newname { $_.name.replace(\"dgo\",\"arv\") }\r\n\r\n// rename using mapping list\r\nIf there’re varying patterns or people like to create a mapping list\r\nusing Excel instead, with the OLD_NAME next to the NEW_NAME in two\r\ncolumns, we can use that mapping as well in a tab separated\r\nTXT/ TSV file (e.g. rename.tsv) to batch rename files.\r\nThe code is basically adding the mv function again to each\r\nline and piping that mv -vi \"OLD_NAME\" \"NEW_NAME\"; back\r\ninto bash for execution:\r\n\r\nsed 's/^/mv -vi \"/;s/\\t/\" \"/;s/$/\";/' < rename.tsv | bash -\r\n\r\nThe double quotes ( ” ) allow for unexpected white spaces in the file\r\nnames - better be safe but sorry.\r\n\\(^_^)\r\n\r\n\r\n\r\n",
    "preview": "posts/2017-08-24-howto-batch-rename-on-the-command-line/manrename.png",
    "last_modified": "2022-09-01T10:42:11+02:00",
    "input_file": {},
    "preview_width": 640,
    "preview_height": 320
  },
  {
    "path": "posts/2016-04-14-open-source-ocr-on-bash-with-imagemagick-and-tesseract/",
    "title": "Open source OCR on bash with ImageMagick and Tesseract",
    "description": "Handy script to ocr PDFs on the commandline with nothing but the FOSS tools ImageMagick and tesseract.",
    "author": [
      {
        "name": "Olaf Schmalfuß",
        "url": "https://www.datamercs.net"
      }
    ],
    "date": "2016-04-14",
    "categories": [],
    "contents": "\r\n\r\nContents\r\n// Introduction\r\n// Installation on cygwin\r\n// OCR my PDF script\r\n\r\n// Introduction\r\nIf customers, colleagues or sales partners throw a PDF at you, usually an invoice with a list of titles, and want a record set for that, we usually assume that at least the PDF is full-text and lets us copy and paste (or import into R↗), so that we can retrieve the list of ISBNs from that for further processing.\r\nWho would believe that there are still scanned invoices being provided, for which neither party has the time nor musings to manually type the ISBNs off of that, to hand over a more convenient list to your friendly neighbourhood librarian?\r\nTruth is, it happens more often than not, which is why we needed to find a way to OCR (i.e. Optical Character Recognition) such files, ideally without any costly, proprietary software like Abbyy FineReader, but simply in bash.\r\nIn come ImageMagick↗ and tesseract↗, two free and open source↗ solutions, which we can wrap into a handy script.\r\nAltogether, this is a simplified implementation of https://ubuntuforums.org/showthread.php?t=880471\r\n// Installation on cygwin\r\nSimply install https://cygwin.com/packages/summary/ImageMagick.html and the desired tesseract language package(s) tesseract-ocr-deu, tesseract-ocr-eng, https://cygwin.com/packages/summary/tesseract-ocr-languages-src.html, as base tesseract-ocr is a dependency anyway.\r\nSame on your favourite package manager in Linux.\r\n// OCR my PDF script\r\n\r\n\r\nShow code\r\n#!/bin/bash\r\n#title:       ocr_my_pdf_v02.sh\r\n#description:   OCRs PDFs to txt, simplified implementation of http://ubuntuforums.org/showthread.php?t=880471\r\n#dependencies: imagemagick, tesseract\r\n#invocation:  ./ocr_my_pdf_v02.sh YOUR-PDF-HERE.pdf\r\n\r\nDPI=300\r\nTESS_LANG=deu+eng # use multiple languages together for recognition, switch order to set primary language\r\n\r\nINPUT_FILENAME=${@%.pdf}\r\nSCRIPT_NAME=$(basename \"$0\" .sh)\r\nTMP_DIR=${SCRIPT_NAME}-tmp\r\nOUTPUT_FILENAME=${INPUT_FILENAME}-ocrd@DPI${DPI}\r\n\r\nmkdir ${TMP_DIR}\r\ncp ${@} ${TMP_DIR}\r\ncd ${TMP_DIR}\r\n\r\nconvert -density ${DPI} -depth 8 -alpha Off ${@} \"${INPUT_FILENAME}.tif\"\r\n#convert -resize 300% ${@} \"${INPUT_FILENAME}.tif\"\r\ntesseract \"${INPUT_FILENAME}.tif\" \"${OUTPUT_FILENAME}\" -l ${TESS_LANG} --psm 6\r\n\r\nmv ${OUTPUT_FILENAME}.txt ..\r\ncd ..\r\nrm -rf ${TMP_DIR}\r\n\r\nHere’re some further options to play with\r\n\r\n\r\nShow code\r\n\r\n# OCR #\r\n#1. enlarge! (ImageMagick)\r\nconvert index.png -resize 300% index2.png\r\n#2. OCR (tesseract)\r\ntesseract index2.png index2.txt\r\n\r\n# tesseract page segmentation mode --psm\r\n#0 = Orientation and script detection (OSD) only.\r\n#1 = Automatic page segmentation with OSD.\r\n#2 = Automatic page segmentation, but no OSD, or OCR\r\n#3 = Fully automatic page segmentation, but no OSD. (Default)\r\n#4 = Assume a single column of text of variable sizes.\r\n#5 = Assume a single uniform block of vertically aligned text.\r\n#6 = Assume a single uniform block of text.\r\n#7 = Treat the image as a single text line.\r\n#8 = Treat the image as a single word.\r\n#9 = Treat the image as a single word in a circle.\r\n#10 = Treat the image as a single character.\r\n\r\nAlso, might consider doing something with getopts here, seehttps://wiki.bash-hackers.org/howto/getopts_tutorialhttps://stackoverflow.com/questions/16483119/an-example-of-how-to-use-getopts-in-bash\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-04-08T23:47:51+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-11-16-rda-the-librarians-double-standard/",
    "title": "RDA - the librarian's (double) standard",
    "description": "While librarians call for openess when it comes to standards and access - the current standard is de facto closed, i.e. hidden behind a pay wall. Nevertheless we'll try implenting and look behind the scenes in this workshop report.",
    "author": [
      {
        "name": "Olaf Schmalfuß",
        "url": "https://www.datamercs.net"
      }
    ],
    "date": "2015-11-16",
    "categories": [],
    "contents": "\r\n\r\nContents\r\n// Introduction\r\n// RDA transition at DG\r\n// looking into the new\r\nrules\r\n// what’s new in RDA\r\n\r\n// Critics\r\n// further reading\r\n\r\n// Introduction\r\nDe Gruyter is also the home of the Library and Information Science\r\nimprint Saur, and of the IFLA\r\nPublications. Thus, we have quite a name and track-record in the\r\nlibrary community, which is why we are very much dedicated to supporting\r\nlibraries in managing their collections by providing fast and\r\nunrestricted access to high quality metadata.\r\nThis also meant (and means) for my work, when taking over and\r\nre-designing De Gruyter’s complete MARC workflow in 2013, that we not\r\nonly have to follow, but stay at the fore front and lead the way when it\r\ncomes to new standards – or the adoption of it!\r\nAnd in comes RDA (“Resource\r\nDescription and Access“) as the facto successor of the RAK-WB (as well as AARC2 for\r\nthe English speaking world), for which De Gruyter collaborates with the\r\nAmerican Library Association (ALA) on publishing and distributing the\r\nnew standard as well its RDA Toolkit 1\r\nRDA is being developed by the Joint Steering Committee for\r\nDevelopment of RDA (JSC) as part of its strategic plan (2005-2009) to\r\nreplace the Anglo-American Cataloguing Rules, 2nd Edition Revised\r\n(AACR2), which were first published in 1978, basically with an\r\n“AARC3”.\r\nAt the same time, the German Standards Committee decided to\r\ninternationalise German standards back in late 2004, including switching\r\nover to MARC 21 and actively participating in RDA’s genesis process. And\r\nso, the German National Library (DNB) is currently working on a German\r\ntranslation for German-speaking countries.\r\nThe decision to implement RDA in the USA has been recommended for 2013.\r\nThe Library of Congress, National Library of Medicine, and National\r\nLibrary of Agriculture will fully implement RDA on March 31, 2013.\r\nThe RDA was created to meet the demands of today’s digital\r\nenvironment as well as align with international data models, such as Functional Requirements for Bibliographic Records (FRBR) and\r\nthe Functional Requirements for Authority Data (FRAD),\r\nand provide the rule set that librarians use to catalogue library items\r\nand to create MARC records.\r\n// RDA transition at DG\r\nTwo terms that are important to differentiate are content\r\nstandards and encoding standards:\r\nContent standards are the rules and regulations, such as\r\nthe Anglo-American Cataloguing Rules (AACR2) or RAK-WB, and this is\r\nwhat’s changing to RDA!\r\nEncoding standards are the implementations of these rules.\r\nExamples are MARC21, Dublin Core, and XML.\r\nThus, in preparation for the transition to RDA, De Gruyter would have\r\nto ensure that our IT infrastructure could handle the additional MARC\r\nfields. Furthermore, those cataloguing De Gruyter’s books would need to\r\nbecome familiar with RDA rules.\r\nRDA can be implemented in various forms/ encoding standards, but in the\r\nLibrary environment, it will still be implemented using MARC, as it had\r\nto be kept compatible to existing AARC databases.\r\nAnd so, the packaging, i.e. MARC21 will stay the same – it “just” needs\r\nsome adjustments on the inside.\r\nAt first glance, there were some concerns that updating De Gruyter’s\r\nMARC records to RDA compliant MARC records would be the most resource\r\nintensive endeavour out of the transition.\r\nTwo possible options that were proposed, to either copy catalogue from\r\nLoC or remap the current MARC records to the new RDA MARC records.\r\nYet, since I had already gone through the endeavour of updating our MARC\r\nrecords for a large consortium deal and overhauling the complete process\r\nof creation altogether, the option of simply “patching” the existing\r\nrecords and building in the new rule set into the process turned out to\r\nbe the easier of the two solutions. Because, looking into what actually\r\ndid change and needed enhancement, when it comes to RDA, was frankly\r\nrather negligible.\r\n// looking into the new rules\r\nThe RDA Toolkit is being\r\npromoted by LoC as an easy method to explore RDA, but which is also one\r\nof the major critique points, when it comes to the philosophy of\r\nsupposedly open standards and the barrier to adoption.\r\nThe RDA Toolkit provides the text of RDA in a searchable format, AACR2\r\nin a searchable format, procedural documentation on how to modify\r\nworkflows when transitioning to RDA, and RDA mapping to MARC and other\r\nencoding standards.\r\nFurther, training material are available on the Library of Congress\r\nwebsite (https://www.loc.gov/aba/rda/) as well as a mapping list\r\nof the elements in RDA and their mapping to MARC: https://www.loc.gov/aba/rda/pdf/core_elements.pdf\r\nThese standards will not replace those that govern the MARC record\r\nstructure, but “simply” several new fields will be added.\r\n// what’s new in RDA\r\n… as compared to the current standard, i.e. RAK-WB in our case? It’s not\r\nthat a title isn’t a title any longer, an author an author or a\r\ncopyright year a copyright year!\r\nAt first glance, RDA simply contains more\r\ninformation than the German RAK-WB, i.e.\r\nmore entities\r\nunlimited number of authors/responsibilities\r\nqualifiers, to identify contributors’ roles or the nature of the\r\ncontent/ media types. An example of the additions are (336) for RDA\r\ncontent types, (337) for RDA media types, and (338) for RDA carrier\r\ntypes. These fields are used to describe materials further and will\r\nreplace the General Material Designators.\r\nsubject headings and classification become mandatory\r\ncataloguing stays in the language of the work\r\noption to include extensive table of contents\r\nAnd further, RDA appears to be even less strict altogether and\r\nprovides more freedom to the cataloguer, as we can stay true to the\r\nsource, i.e. the title page and do not necessarily have to “normalize”\r\n(but could do that if we chose to in additional fields), which is a\r\nplus, when it comes to data conversion.\r\nBecause let’s be true, the masses of newly created eBooks are not\r\ncatalogued one by one manually from the title page, which we sometimes\r\nwished we could do, but the data from the ERP system will be broken down\r\nand converted into the respective MARC fields, with some additional\r\nquality (or sanity checks) later on.\r\nAnd so, here’s what needed to be added to our DG\r\nMARC core set:\r\n=040  \\\\$aDE-B1597$beng$cDE-B1597$erda\r\n=336  \\\\$atext$btxt$2rdacontent\r\n=337  \\\\$acomputer$bc$2rdamedia\r\n=338  \\\\$aonline resource$bcr$2rdacarrier\r\n=347  \\\\$atext file$bPDF$2rda\r\n=505  00 literal structured TOCs for further search\r\nentries (basically SEO) instead of the links to the PDFs\r\nRelators for responsibilities: http://id.loc.gov/vocabulary/relators\r\nSo from a practical perspective, what’s the benefit for simply adding\r\nthe new fields and just replacing coded data fields for plain text\r\nones?\r\nActually, I am not sure either, but with more linking introduced, we\r\nmight want to re-focus our cataloguing away from a record based logic\r\nand towards based on entities and relationships, thinking in graphs,\r\nwhich will eventually lead into BibFrame, the next standard on\r\nthe horizon.\r\n// Critics\r\nThere has been some opposition to the transition stating that RDA is\r\nnot a robust enough standard. In response, the three national libraries\r\nimplemented a test of RDA at various institutions. After which those who\r\nreceived RDA training were asked whether there should be a transition\r\nand roughly 70% said yes with some changes.\r\nFurther, while as a profession, librarians continue to push for open\r\naccess, open data and the free exchange of information, the new\r\nbibliographic standard to replace AACR2 was released basically as a\r\nclosed standard, sitting behind a subscription\r\npaywall!\r\nThat is the antithesis to opening the library data through datasets and\r\nAPIs, especially when other open (publicly available) standards are\r\nquite common in the library sphere already, e.g.\r\nDublin Core\r\nISBD\r\nKBART\r\nMADS\r\nMARC21\r\nMODS\r\nONIX\r\nRDF\r\nXML\r\nWhile the RDA vocabularies might be openly available (if you can find\r\nthem here), for developers, they are\r\nessentially useless without the standards documents that give them\r\nmeaning.\r\nThis massively hinders not only librarians in developing countries,\r\nbut also the adoption of the new standard in general!\r\n… a true (double) standard - let’s see where it goes…\r\n// further reading\r\nFor more information in German, refer also to Heidrun Wiesenmüller’s\r\npage to the textbook “Basiswissen RDA”: https://www.basiswissen-rda.de/\r\n\r\n > Publisher De Gruyter\r\ncollaborates with the American Library Association (ALA) on publishing\r\nand distributing the new standard RDA: Resource Description and Access\r\nFebruary 1, 2012 De Gruyter and the ALA have signed an agreement,\r\naccording to which the publisher will be responsible for the publication\r\nand global distribution of the German-language version of the new set of\r\ncataloguing standards for print and digital media in libraries and\r\nbeyond, RDA: Resource Description and Access. The publisher will also\r\nsell licenses for the multilingual online version RDA Toolkit in\r\nGerman-speaking countries. Major international libraries and library\r\nassociations in the USA, Canada, Australia and Great Britain have been\r\npushing for the introduction of RDA, which is expected to be recognised\r\ninternationally as the successor of the Anglo-American Cataloguing Rules\r\n(AACR2) and meet the demands for a much more tightly networked\r\ninformation landscape. (https://www.degruyter.com/dg/newsitem/13/de-gruyter-kooperiert-mit-der-american-library-association-ala-der-verlag-wird-das-neue-regelwerk-rda-resource-description-and-access-herausgeben-und-vertreiben)↩︎\r\n",
    "preview": {},
    "last_modified": "2022-05-30T09:49:28+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-02-02-working-with-kbart-files/",
    "title": "Working with KBART files",
    "description": "KBART files are standardized title lists in .TXT format, that specify electronic collections. This is to provide a short overview on what to expect from this metadata format and how to work with it as publisher or librarian.",
    "author": [
      {
        "name": "Olaf Schmalfuss",
        "url": "https://www.datamercs.net"
      }
    ],
    "date": "2015-02-03",
    "categories": [],
    "contents": "\r\n\r\nContents\r\n// Introduction\r\n// What is a knowledge base?\r\n\r\n// KBART\r\nat De Gruyter\r\n// Working\r\nwith KBART\r\n// how to open a KBART file\r\n(with Excel)\r\n// how to customzie,\r\ni.e. filter a KBART file\r\n\r\n\r\n// Introduction\r\nKBART, which stands for “Knowledge Bases and Related Tools,” is a NISO Recommended\r\nPractice↗.\r\nVery simply, KBART recommends best practices for the communication of\r\nelectronic resource title list and coverage data from content providers\r\nto knowledge base (KB) suppliers. KBART specifies file format, delivery\r\nmechanisms and fields to include, and it applies to both serials and\r\nmonographs.\r\nThus, a single KBART file is basically a standardized list, separated by\r\nTAB in .TXT format.\r\nBy specification KBARTs are not intended for rich bibliographic\r\ndescription, but work in tandem together with MARC records and chunk\r\nlevel data like BITS and JATS XML to provide the collection level\r\ninformation for full scale title information and discoverability within\r\nour model of the trinity of\r\ndiscovery↗.\r\n// What is a knowledge base?\r\nA knowledge base is an extensive database maintained by a knowledge\r\nbase supplier that contains information about electronic resources such\r\nas title lists, coverage dates, inbound linking syntax, etc. Knowledge\r\nbases typically organize the resources provided by a content provider\r\ninto collections or databases that reflect specific content provider\r\nofferings, for example packages of e-journals, e-books, or other\r\nmaterials. Knowledge bases can be customized by individual institutions\r\nto reflect their local collections.\r\nKnowledge bases are used to provide data for OpenURL link resolvers and\r\nto populate library discovery systems with an institution’s e-resource\r\nholdings data. Many libraries also use knowledge base data in library\r\ncatalogues, for e-journal title lists, in electronic resource management\r\nsystems (ERMs), and in other tools.\r\nKnowledge base suppliers ingest the data into their KBs, and libraries\r\nthen select the packages and titles that they have access to.\r\nLibraries can also use KBART to create files for custom local and\r\nconsortial packages and load them into their local knowledge bases. Some\r\nlibrarians also find KBART title list files useful for understanding a\r\ncontent provider’s offerings or determining the contents of a standard\r\npackage they have purchased.\r\nsee\r\nalso KBART FAQ↗\r\n// KBART at De Gruyter\r\nare being created for\r\nall past and present eBook packages\r\nincluding certain customised consortia packages\r\nexcept those for internal usage only (>>> blacklist)\r\n\r\nall active eJournal packages\r\nall databases, in one database KBART file\r\nOpen Access KBARTs, one for eBooks one for eJournals\r\ntwo collection files, one for eBooks one for eJournals\r\none KBART for all “out of print” eBooks\r\nThe workflow and toolchain I created for that will be laid out, as\r\nfar as possible, at a later stage, bringing together data from multiple\r\nsources, utilizing SQL, bash and RStudio, loading the data to all\r\nrelevant data recipients, i.e. KBs and end customers etc. via FTP and\r\nemail, and host the data on a customized metadata portal for public\r\naccess and self-service1.\r\nThis development was necessary after switching from a couple of fixed\r\nfront list eBook packages a year to more and more complex and dynamic\r\nproducts, like PDA/ EBA and publisher partners’ packages.\r\n// Working with KBART\r\n// how to open a KBART file\r\n(with Excel)\r\nWhile this question seems a little odd, about how to open a simple\r\n.TXT file, in reality it appears that the common librarian, or sales or\r\nmarketing person, is actually not necessarily that used to working with\r\n.TXTs, that I got so many questions on how to read or use such a file in\r\nExcel, that I even might make an instructional video about that, just\r\nfor reference.\r\nYet admittedly, there might be the one or other obstacle that we could\r\naddress here already, on …\r\n// … the art of\r\nhow to open a KBART file in Excel\r\nafter downloading the .TXT file\r\nright click, open with\r\ndrag into (closed) Excel, i.e. onto the icon\r\nnot possible with the Excel icon…\r\n… but working with LibreOffice\r\n\r\ndrag into (open) Excel\r\nopen in notepad, then copy&paste all into Excel (which is what I\r\nusually do)\r\nopen from inside Excel\r\nimport from inside Excel\r\nrename .txt to .xls\r\nBUT “dragging&dropping” into as well as “right-click, open-with”\r\nExcel poses a special, UTF-8 related problem, as Excel is not smart\r\nenough to recognize the encoding this way, and naturally, we are dealing\r\nwith lots of German, French and Greek characters in scientific\r\nliterature, which causes special characters and Umlauts to get\r\nmangled in Excel.\r\nA solution for that would be to convert or encode the KBART file to\r\nUTF-8 WITH BOM first, for example in notepad++↗\r\nAnd while according to the Unicode standard the use of a “Byte Order\r\nMark” (BOM) is neither required nor recommended for UTF-82, it\r\ncan be crucial for UTF-8 recognition in Excel, and may make all the\r\ndifference between Gibberish and German.\r\nThus, it’s “allowed” in contexts where a particular protocol\r\n(e.g. Microsoft conventions for .txt files) may require use of the BOM\r\non certain Unicode data streams, such as KBART/.TXT files. When you need\r\nto conform to such a protocol, use a BOM3.\r\nLibreOffice↗ on the other\r\nhand just works, yet won’t open right away, but will ask first.\r\nFor me the easiest solution is to open in notepad++ first, the\r\ncopy&paste all into Excel, making sure that only\r\nTAB is used as delimiter, et voilà!\r\n// how to customzie,\r\ni.e. filter a KBART file\r\n// filter KBART against a\r\nlist of ISBNs\r\nFor pick&choose collections, where no off-the-shelf package and\r\nthus no dedicated KBART file is available, we’ll filter the complete\r\nKBART file based on an ISBN list, with each ISBN on its own line and EOL\r\nin UNIX format, i.e. \\n, e.g. ‘filter_file.txt’.\r\nAlso, we will keep one of the column names from the header in the first\r\nline, to grep the header of the KBART as well,\r\ne.g. ‘ONLINE_IDENTIFIER’\r\n\r\n# our 'filter_file.txt' looks as follows:\r\n$ cat filter_file.txt\r\nONLINE_IDENTIFIER\r\nISBN1\r\nISBN2\r\nISBN3\r\n...\r\n..\r\n.\r\n\r\n# assuming the complete KBART is in the same folder as the filter_file, we'll use 'LC_ALL=C fgrep' for faster execution:\r\n$ LC_ALL=C fgrep -i -f filter_file.txt degruyter_global_ebooks_* > DG_KBART_$(date +%F).txt\r\n\r\n\r\nhttps://oschmalfuss.github.io/degruyter/FTP-Index.html\r\n↗↩︎\r\nhttps://www.unicode.org/versions/Unicode12.1.0/ch02.pdf#G27981\r\n↗↩︎\r\nhttps://www.unicode.org/faq/utf_bom.html#bom9 ↗↩︎\r\n",
    "preview": "posts/2015-02-02-working-with-kbart-files/nisokbart.png",
    "last_modified": "2022-09-01T10:46:30+02:00",
    "input_file": {},
    "preview_width": 491,
    "preview_height": 272
  },
  {
    "path": "posts/2013-07-21-a-rough-guide-to-cold-brew/",
    "title": "A rough guide to cold brew",
    "description": "While to the common librarian Cory Doctorow is merely known as champion for Open Access, the Creative Commons, the EFF etc., he is also a hell of a barrista, introducing many a young scholar - the author included - to the carnality of cold brew.  \nSo, here is the rough guide for future reference.",
    "author": [
      {
        "name": "Olaf Schmalfuß",
        "url": "https://www.datamercs.net"
      }
    ],
    "date": "2013-07-21",
    "categories": [],
    "contents": "\r\n// essential reading\r\nHOWTO\r\nattain radical hotel-room coffee independence\r\neasier\r\nCheap,\r\neasy, no-mess cold-brew coffee\r\nbut frankly: simply use your French Press and let the coarse ground\r\ncoffee brew over night in your fridge…\r\nEnjoy!\r\n// A prosaic how-to, or love\r\nletter\r\nas given here\r\nto its best by Cory Doctorow, in\r\n\r\nHomeland\r\n\r\n\r\nYou’ve had hot coffee before, and in the hands of a skilled maker,\r\ncoffee can be amazing. But the fact is that coffee is one of the hardest\r\nthings to get right in the world. Even with great beans and a great\r\nroast and great equipment, a little too much heat, the wrong grind, or\r\nletting things go on too long will produce a cup of bitterness. Coffee’s\r\nfull of different acids, and depending on the grind, temperature, roast,\r\nand method, you can “overextract” the acids from the beans, or overheat\r\nthem and oxidize them, producing that awful taste you get at donut shops\r\nand Starbucks.\r\nBut there is Another Way. If you make coffee in cold water, you only\r\nextract the sweetest acids, the highly volatile flavors that hint at\r\nchocolate and caramel, the ones that boil away or turn to sourness under\r\nimperfect circumstances. Brewing coffee in cold water sounds weird, but\r\nin fact, it’s just about the easiest way to make a cup (or a jar) of\r\ncoffee.\r\nJust grind coffee – keep it coarse, with grains about the size of sea\r\nsalt – and combine it with twice as much water in an airtight jar. Give\r\nit a hard shake and stick it somewhere cool overnight (I used a cooler\r\nbag loaded with ice from ice camp and wrapped the whole thing in bubble\r\nwrap for insulation). In the morning, strain it through a colander and a\r\npaper coffee filter. What you’ve got now is coffee concentrate, which\r\nyou can dilute with cold water to taste – I go about half and half. If\r\nyou’re feeling fancy, serve it over ice.\r\nHere’s the thing: cold-brew coffee tastes amazing, and it’s practically\r\nimpossible to screw it up. Unlike espresso, where all the grounds have\r\nto be about the same size so that the high pressure water doesn’t cause\r\nfracture lines in the “puck” of coffee that leave some of the coffee\r\nunextracted and the rest overextracted, cold-brew grounds can be just\r\nabout any size. Seriously, you could grind it with a stone axe. Unlike\r\ndrip coffee, which goes sour and bitter if you leave the grounds in\r\ncontact with the water for too long, cold-brew just gets yummier and\r\nyummier (and more and more caffeinated!) the longer the grounds sit in\r\nthe water. Cold-brewing in a jar is pretty much the easiest way to make\r\ncoffee in the known universe – if you don’t mind waiting overnight for\r\nthe brew – and it produces the best-tasting, most potent coffee you’ve\r\never drunk. The only downside is that it’s kind of a pain in the ass to\r\nclean up, but if you want to spend some more money, you can invest in\r\nvarious gadgets to make it easier to filter the grounds, from cheap\r\nlittle Toddy machines all the way up to hand-blown glass “Kyoto\r\ndrippers” that look like something from a mad scientist’s lab. But all\r\nyou need to make a perfectly astounding cup of cold-brewed jet fuel is a\r\nmason jar, coffee, water, and something to strain it through. They’ve\r\nbeen making iced coffee this way in New Orleans for centuries, but for\r\nsome unknown reason, it never seems to have caught on big-time.\r\n[…]\r\nIt’s funny watching someone take a sip of cold-brew for the first time,\r\nbecause it looks and smells strong, and it is, and coffee drinkers have\r\nbeen trained to think that “strong” equals “bitter.” The first mouthful\r\nwashes over your tongue and the coffee flavor wafts up the back of your\r\nthroat and fills up your sinus cavity and your nose is all, “THIS IS\r\nINCREDIBLY STRONG!” And the flavor is strong, but there isn’t a hint of\r\nbitterness. It’s like someone took a cup of coffee and subtracted\r\neverything that wasn’t totally delicious, and what’s left behind is a\r\npure, powerful coffee liquor made up of all these subtle flavors: citrus\r\nand cocoa and a bit of maple syrup, all overlaid on the basic and\r\npowerful coffee taste you know and love.\r\n\r\n(CC BY-NC-ND 3.0)\r\n\r\n\r\n\r\n",
    "preview": "posts/2013-07-21-a-rough-guide-to-cold-brew/cold_brew_cats.jpg",
    "last_modified": "2022-09-01T10:49:00+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-05-23-marcedit-under-ubuntu-linux/",
    "title": "MarcEdit under Ubuntu Linux",
    "description": "HowTo install MarcEdit under Ubuntu 13.04, via Mono or Wine",
    "author": [
      {
        "name": "Olaf Schmalfuß",
        "url": "https://www.datamercs.net"
      }
    ],
    "date": "2013-05-23",
    "categories": [],
    "contents": "\r\noriginally posted here\r\n// MarcEdit via MONO\r\nTerry Reese’s only install instructions for Linux on his Worklog: http://blog.reeset.net/archives/805. The mentioned\r\n“Install.txt” does not even exist in the latest iteration of MarcEdit,\r\nsee the relevant part below.\r\nThere is also a tutorial on Youtube: https://www.youtube.com/watch?v=N65IHRiRby8\r\nThe following set-up was done on Ubuntu 13.04\r\n\r\n# install MonoDevelop (http://monodevelop.com/)\r\nsudo apt-get install monodevelop\r\n\r\n# create you MARC records working directory\r\nmkdir MarcRecords; cd MarcRecords\r\n\r\n# download the latest version of MarcEdit from http://marcedit.reeset.net/downloads and unzip\r\nwget http://marcedit.reeset.net/software/marcedit_other.zip; unzip marcedit_other.zip; rm marcedit_other.zip\r\n\r\n# following the MarcEdit install instructions run the following command once (also check other dependencies: http://blog.reeset.net/archives/805):\r\nmono ~/MarcRecords/marcedit_linux/linux_bootloader.exe\r\n\r\n# a permanent alias makes it easier to start MarcEditin the future\r\necho \"alias marcedit='mono ~/MarcRecords/marcedit_linux/MarcEdit.exe'\" >> ~/.bash_aliases\r\n\r\n# restart Terminal in order to activate the alias\r\n\r\n# start MarcEdit.exe via alias \"marcedit\"\r\nmarcedit\r\n\r\n# or start MarcEdit.exe via\r\nmono ~/MarcRecords/marcedit_linux/MarcEdit.exe\r\n\r\nFrom Terry Reese’s “MarcEdit Installation Instructions”:\r\n\r\nInstall.txt Last Modified: 12/28/2009\r\n\r\n\"LINUX/OTHER INSTALLATION PROCEDURE:  \r\n\r\n1.1  INSTALLATION FROM ZIP  \r\n\r\na) Ensure that the dependencies have been installed  \r\n   1) Dependency list:  \r\n      i) MONO 2.4+ (Runtime plus the System.Windows.Forms library [these are sometimes separate])  \r\n     ii) YAZ 3 + YAZ 3 develop Libraries + YAZ++ ZOOM bindings  \r\n    iii) ZLIBC libraries  \r\n     iV) libxml2/libxslt libraries  \r\nb) Unzip marcedit.zip  \r\nc) Navigate to the MarcEdit program directory and run linux_bootloader.exe (example, mono linux_bootloader.exe)  \r\nd) Yaz.Sharp.dll.config — ensure that the dllmap points to the correct version of the shared libyaz object.  \r\ne) main_icon.bmp can be used for a desktop icon  \r\nf) On first run:  \r\n   a) mono MarcEdit.exe  \r\n   b) Preferences tab will open, click on other, and set the following two values:  \r\n      i) Temp path: /tmp/  \r\n     ii) MONO path: [to your full mono path; likely /usr/bin/mono]\"  \r\n// MarcEdit via WINE\r\n\r\nsudo apt-get install wine\r\n\r\nWINEPREFIX='/home/USERNAME/wine32' WINEARCH='win32' wine 'wineboot'\r\nWINEPREFIX='/home/USERNAME/wine32' bash winetricks dotnet40 corefonts\r\n\r\nwget http://marcedit.reeset.net/software/MarcEdit_Setup.msi\r\n\r\nWINEPREFIX='/home/USERNAME/wine32' msiexec /i MarcEdit_Setup.msi\r\n\r\nrm MarcEdit_Setup.msi\r\n\r\necho \"alias winmarc='WINEPREFIX='/home/USERNAME/wine32' wine /home/USERNAME/wine32/drive_c/Program\\ Files/MarcEdit\\ 5.0/MarcEdit.exe'\" >> ~/.bash_aliases\r\n\r\n# restart Terminal\r\n\r\n# start MarcEdit.exe via alias \"winmarc\"\r\nwinmarc\r\n\r\n# or start MarcEdit.exe via\r\nWINEPREFIX='/home/USERNAME/wine32' wine /home/USERNAME/wine32/drive_c/Program\\ Files/MarcEdit\\ 5.0/MarcEdit.exe\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-09-01T10:48:27+02:00",
    "input_file": {}
  }
]
